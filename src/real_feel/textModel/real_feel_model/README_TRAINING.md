# CLS + MaxPool Ensemble Training for EC2

Production-ready training setup for the CLS + MaxPool ensemble bot detection model.

## ğŸ¯ Quick Start

### Option 1: Docker (Recommended for EC2)
```bash
./launch_training.sh docker production /path/to/dataset.csv ./trained_models
```

### Option 2: Poetry (Local Development)
```bash
./launch_training.sh poetry production /path/to/dataset.csv ./trained_models
```

### Option 3: Docker Compose
```bash
export DATA_PATH=/path/to/your/data
export DATASET_FILE=bot_dataset.csv
export CONFIG=production
docker-compose -f docker-compose.training.yml up
```

## ğŸ“Š Dataset Support

### ğŸ¯ Cresci-2017 Dataset (Built-in)
The training script automatically loads the **Cresci-2017 bot detection dataset** included in the project:

**Structure**:
```
../datasets/datasets_full.csv/
â”œâ”€â”€ genuine_accounts.csv/tweets.csv     # Human tweets (label=0)
â”œâ”€â”€ fake_followers.csv/tweets.csv       # Bot tweets (label=1)
â”œâ”€â”€ social_spambots_1.csv/tweets.csv    # Bot tweets (label=1)
â”œâ”€â”€ social_spambots_2.csv/tweets.csv    # Bot tweets (label=1)
â”œâ”€â”€ social_spambots_3.csv/tweets.csv    # Bot tweets (label=1)
â”œâ”€â”€ traditional_spambots_1.csv/tweets.csv # Bot tweets (label=1)
â”œâ”€â”€ traditional_spambots_2.csv/tweets.csv # Bot tweets (label=1)
â”œâ”€â”€ traditional_spambots_3.csv/tweets.csv # Bot tweets (label=1)
â””â”€â”€ traditional_spambots_4.csv/tweets.csv # Bot tweets (label=1)
```

**Dataset Stats**:
- **~37K human tweets** from genuine accounts
- **~100K+ bot tweets** across 8 bot categories
- **Total: ~137K samples** with balanced representation

### ğŸ“ Custom Dataset Format
For custom datasets, use CSV with `text` and `label` columns:

```csv
text,label
"I love this amazing product! Great quality and fast shipping.",0
"BUY NOW!!! CLICK HERE FOR FREE MONEY!!! ğŸš€ğŸš€ğŸš€",1
"Just finished reading an interesting research paper on neural networks.",0
"URGENT!!! Make $1000/day from home! Click link now!!!",1
```

**Labels**:
- `0` = Human/Legitimate
- `1` = Bot/Spam

## ğŸš€ EC2 Deployment

### 1. Prepare EC2 Instance
```bash
# Recommended: p3.2xlarge or g4dn.xlarge with Deep Learning AMI
# Minimum: 16GB RAM, 100GB storage, GPU support
```

### 2. Transfer Files
```bash
# Copy project to EC2
scp -r . ec2-user@your-instance:/home/ec2-user/realfeel-training/

# Upload dataset
scp your_dataset.csv ec2-user@your-instance:/home/ec2-user/data/
```

### 3. Run Training
```bash
ssh ec2-user@your-instance
cd /home/ec2-user/realfeel-training
./launch_training.sh docker production /home/ec2-user/data/your_dataset.csv ./trained_models
```

## âš™ï¸ Configuration Options

### Training Configs
- **`fast`**: Quick experimentation (6 layers, 5 epochs)
- **`production`**: Full performance (12 layers, 20 epochs)
- **`default`**: Balanced setup (9 layers, 10 epochs)

### Command Line Options
```bash
python train_ensemble.py \
  --config production \
  --data_path /path/to/dataset.csv \
  --output_dir ./trained_models \
  --epochs 25 \
  --batch_size 64 \
  --learning_rate 1e-4 \
  --gpu_id 0 \
  --experiment_name custom_experiment
```

## ğŸ“ˆ Expected Training Times

| Dataset Size | GPU (p3.2xlarge) | CPU Only |
|-------------|------------------|----------|
| 10K samples | ~30 minutes     | ~4 hours |
| 100K samples| ~3 hours        | ~24 hours|
| 1M samples  | ~12 hours       | ~7 days  |

## ğŸ’¾ Output Files

Training produces:
- **`ensemble_TIMESTAMP_best.pt`**: Trained model weights
- **`training_results.json`**: Performance metrics and config
- **`training_TIMESTAMP.log`**: Detailed training logs

## ğŸ“‹ Model Performance

The ensemble combines:
- **CLS Model (70%)**: Sophisticated bot detection
- **MaxPool Model (30%)**: Obvious spam detection

Expected performance on balanced datasets:
- **Accuracy**: 85-92%
- **F1 Score**: 0.83-0.90
- **ROC AUC**: 0.88-0.95

## ğŸ”§ Dependencies

Using Poetry and Docker with:
- **Python**: 3.11+
- **PyTorch**: 2.8.0+
- **Transformers**: 4.55.0+
- **CUDA**: 11.8+ (for GPU training)

## ğŸ“ Usage Examples

```bash
# Fast development training
./launch_training.sh poetry fast ./small_dataset.csv ./models

# Production training with custom parameters
./launch_training.sh docker production ./large_dataset.csv ./models 0

# Monitor GPU during training
watch -n 1 nvidia-smi

# Check training progress
tail -f ./trained_models/training_*.log
```

## ğŸ¯ Integration

After training, use the model in the RealFeel pipeline:

```python
from real_feel.ensemble_bot_detector import create_ensemble_bot_detector

# Load trained model
detector = create_ensemble_bot_detector(
    model_path="./trained_models/ensemble_20231215_143022_best.pt"
)

# Use in pipeline
result = detector.check_bot(user_id="12345", tweet_text="Sample text")
```

## ğŸ›¡ï¸ Production Notes

- Use Docker for consistent deployment across environments
- Monitor GPU memory usage during training
- Save checkpoints for long training runs
- Validate dataset quality before training
- Test model performance on held-out validation set

---

**Ready to train your bot detection model!** ğŸ¤–âœ¨