{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: Gradient Descent Optimization Strategy\n",
    "**Advanced Optimization Techniques for Transformer Training**\n",
    "\n",
    "This experiment focuses on implementing and comparing advanced optimization strategies:\n",
    "1. **AdamW Optimizer**: Decoupled weight decay for better regularization\n",
    "2. **Learning Rate Scheduling**: Warmup + decay strategies for stable training\n",
    "3. **Gradient Clipping**: Prevention of exploding gradients in deep networks\n",
    "4. **Optimization Strategy Comparison**: Empirical evaluation of different approaches\n",
    "\n",
    "**Theoretical Foundation**: Transformers require careful optimization due to their depth and attention mechanisms. Proper optimization can mean the difference between successful training and divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device and seeds for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Learning Rate Schedulers\n",
    "**Theoretical Background**: Different LR schedules have different theoretical justifications:\n",
    "- **Warmup**: Prevents large gradient updates early in training when weights are random\n",
    "- **Cosine Annealing**: Smooth decay that allows escaping local minima\n",
    "- **Linear Decay**: Simple and effective for most transformer training scenarios\n",
    "- **Polynomial Decay**: Provides more control over decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLRScheduler:\n",
    "    \"\"\"Advanced learning rate schedulers for transformer optimization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "        \"\"\"\n",
    "        Linear warmup followed by linear decay\n",
    "        \n",
    "        Theory: lr = base_lr * min(step/warmup_steps, (total_steps - step)/(total_steps - warmup_steps))\n",
    "        \"\"\"\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            return max(\n",
    "                0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "            )\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, \n",
    "                                       num_cycles=0.5, last_epoch=-1):\n",
    "        \"\"\"\n",
    "        Cosine warmup followed by cosine annealing\n",
    "        \n",
    "        Theory: Smooth transitions help with convergence stability\n",
    "        lr = base_lr * 0.5 * (1 + cos(π * cycles * progress))\n",
    "        \"\"\"\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_polynomial_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, \n",
    "                                          power=1.0, last_epoch=-1):\n",
    "        \"\"\"\n",
    "        Polynomial decay with warmup\n",
    "        \n",
    "        Theory: lr = base_lr * (1 - progress)^power\n",
    "        Power controls decay rate: power=1 is linear, power>1 is faster decay\n",
    "        \"\"\"\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            elif current_step > num_training_steps:\n",
    "                return 0.0\n",
    "            else:\n",
    "                lr_range = float(num_training_steps - num_warmup_steps)\n",
    "                decay_steps = float(current_step - num_warmup_steps)\n",
    "                pct_remaining = 1 - decay_steps / lr_range\n",
    "                return pct_remaining ** power\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_inverse_sqrt_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\n",
    "        \"\"\"\n",
    "        Inverse square root decay (Attention is All You Need paper)\n",
    "        \n",
    "        Theory: lr = base_lr * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
    "        Used in original Transformer paper\n",
    "        \"\"\"\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step == 0:\n",
    "                return 0.0\n",
    "            return min(current_step ** (-0.5), current_step * (num_warmup_steps ** (-1.5)))\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "# Test different schedulers\n",
    "def visualize_lr_schedules(base_lr=2e-4, warmup_steps=1000, total_steps=10000):\n",
    "    \"\"\"Visualize different learning rate schedules\"\"\"\n",
    "    \n",
    "    # Create dummy model and optimizer\n",
    "    dummy_model = nn.Linear(10, 1)\n",
    "    \n",
    "    schedules = {\n",
    "        'Linear': AdvancedLRScheduler.get_linear_schedule_with_warmup,\n",
    "        'Cosine': AdvancedLRScheduler.get_cosine_schedule_with_warmup,\n",
    "        'Polynomial (p=1.5)': lambda opt, w, t: AdvancedLRScheduler.get_polynomial_schedule_with_warmup(opt, w, t, power=1.5),\n",
    "        'Inverse Sqrt': lambda opt, w, t: AdvancedLRScheduler.get_inverse_sqrt_schedule_with_warmup(opt, w)\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for name, scheduler_fn in schedules.items():\n",
    "        optimizer = optim.AdamW(dummy_model.parameters(), lr=base_lr)\n",
    "        scheduler = scheduler_fn(optimizer, warmup_steps, total_steps)\n",
    "        \n",
    "        lrs = []\n",
    "        for step in range(total_steps):\n",
    "            lrs.append(scheduler.get_last_lr()[0])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        plt.plot(lrs, label=name, linewidth=2)\n",
    "    \n",
    "    plt.axvline(x=warmup_steps, color='red', linestyle='--', alpha=0.7, label='End of Warmup')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return lrs\n",
    "\n",
    "# Visualize schedules\n",
    "print(\"Learning Rate Schedule Comparison:\")\n",
    "visualize_lr_schedules(base_lr=2e-4, warmup_steps=1000, total_steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Gradient Clipping Strategies\n",
    "**Theoretical Foundation**: \n",
    "- **Norm Clipping**: Rescales gradients to have maximum norm, preserving direction\n",
    "- **Value Clipping**: Clips individual gradient values, may change direction\n",
    "- **Adaptive Clipping**: Adjusts clipping threshold based on gradient statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedGradientClipper:\n",
    "    \"\"\"Advanced gradient clipping strategies for stable training\"\"\"\n",
    "    \n",
    "    def __init__(self, clip_type='norm', clip_value=1.0, adaptive_factor=0.99):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip_type: 'norm', 'value', 'adaptive_norm', or 'percentile'\n",
    "            clip_value: Clipping threshold\n",
    "            adaptive_factor: EMA factor for adaptive clipping\n",
    "        \"\"\"\n",
    "        self.clip_type = clip_type\n",
    "        self.clip_value = clip_value\n",
    "        self.adaptive_factor = adaptive_factor\n",
    "        self.running_grad_norm = None\n",
    "        self.grad_norm_history = []\n",
    "    \n",
    "    def clip_gradients(self, model):\n",
    "        \"\"\"\n",
    "        Apply gradient clipping to model parameters\n",
    "        \n",
    "        Returns:\n",
    "            dict with clipping statistics\n",
    "        \"\"\"\n",
    "        # Compute current gradient norm\n",
    "        total_norm = 0.0\n",
    "        param_count = 0\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                param_count += param.numel()\n",
    "        \n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        self.grad_norm_history.append(total_norm)\n",
    "        \n",
    "        # Apply clipping based on strategy\n",
    "        if self.clip_type == 'norm':\n",
    "            # Standard gradient norm clipping\n",
    "            clipped_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_value)\n",
    "            was_clipped = clipped_norm > self.clip_value\n",
    "            \n",
    "        elif self.clip_type == 'value':\n",
    "            # Clip individual gradient values\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), self.clip_value)\n",
    "            clipped_norm = total_norm\n",
    "            was_clipped = False  # Hard to determine for value clipping\n",
    "            \n",
    "        elif self.clip_type == 'adaptive_norm':\n",
    "            # Adaptive gradient norm clipping\n",
    "            if self.running_grad_norm is None:\n",
    "                self.running_grad_norm = total_norm\n",
    "            else:\n",
    "                self.running_grad_norm = (self.adaptive_factor * self.running_grad_norm + \n",
    "                                        (1 - self.adaptive_factor) * total_norm)\n",
    "            \n",
    "            adaptive_clip_value = self.clip_value * self.running_grad_norm\n",
    "            clipped_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), adaptive_clip_value)\n",
    "            was_clipped = clipped_norm > adaptive_clip_value\n",
    "            \n",
    "        elif self.clip_type == 'percentile':\n",
    "            # Percentile-based clipping (clip based on gradient norm percentiles)\n",
    "            if len(self.grad_norm_history) > 100:  # Need some history\n",
    "                clip_threshold = np.percentile(self.grad_norm_history[-100:], 95)\n",
    "                clipped_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_threshold)\n",
    "                was_clipped = clipped_norm > clip_threshold\n",
    "            else:\n",
    "                clipped_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_value)\n",
    "                was_clipped = clipped_norm > self.clip_value\n",
    "        \n",
    "        else:\n",
    "            # No clipping\n",
    "            clipped_norm = total_norm\n",
    "            was_clipped = False\n",
    "        \n",
    "        return {\n",
    "            'original_norm': total_norm,\n",
    "            'clipped_norm': clipped_norm.item() if isinstance(clipped_norm, torch.Tensor) else clipped_norm,\n",
    "            'was_clipped': was_clipped,\n",
    "            'param_count': param_count\n",
    "        }\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get gradient norm statistics\"\"\"\n",
    "        if not self.grad_norm_history:\n",
    "            return {}\n",
    "        \n",
    "        norms = np.array(self.grad_norm_history)\n",
    "        return {\n",
    "            'mean_norm': np.mean(norms),\n",
    "            'std_norm': np.std(norms),\n",
    "            'max_norm': np.max(norms),\n",
    "            'min_norm': np.min(norms),\n",
    "            'percentile_95': np.percentile(norms, 95),\n",
    "            'percentile_99': np.percentile(norms, 99)\n",
    "        }\n",
    "    \n",
    "    def plot_gradient_norms(self, window_size=100):\n",
    "        \"\"\"Plot gradient norm evolution\"\"\"\n",
    "        if len(self.grad_norm_history) < 2:\n",
    "            print(\"Not enough gradient norm history to plot\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot raw gradient norms\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.grad_norm_history, alpha=0.6, label='Raw Gradient Norm')\n",
    "        \n",
    "        # Plot moving average\n",
    "        if len(self.grad_norm_history) > window_size:\n",
    "            moving_avg = np.convolve(self.grad_norm_history, np.ones(window_size)/window_size, mode='valid')\n",
    "            plt.plot(range(window_size-1, len(self.grad_norm_history)), moving_avg, \n",
    "                    label=f'Moving Average ({window_size})', linewidth=2)\n",
    "        \n",
    "        plt.axhline(y=self.clip_value, color='red', linestyle='--', label='Clip Threshold')\n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Gradient Norm')\n",
    "        plt.title('Gradient Norm Evolution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot gradient norm histogram\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(self.grad_norm_history, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(x=self.clip_value, color='red', linestyle='--', label='Clip Threshold')\n",
    "        plt.xlabel('Gradient Norm')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Gradient Norm Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test gradient clipper\n",
    "test_model = nn.Linear(100, 10)\n",
    "clipper = AdvancedGradientClipper(clip_type='norm', clip_value=1.0)\n",
    "\n",
    "# Simulate gradient clipping\n",
    "for i in range(10):\n",
    "    # Create fake gradients with increasing magnitude\n",
    "    for param in test_model.parameters():\n",
    "        param.grad = torch.randn_like(param) * (1 + i * 0.5)\n",
    "    \n",
    "    stats = clipper.clip_gradients(test_model)\n",
    "    print(f\"Step {i}: Original norm: {stats['original_norm']:.4f}, \"\n",
    "          f\"Clipped: {stats['was_clipped']}, Final norm: {stats['clipped_norm']:.4f}\")\n",
    "\n",
    "print(\"\\nGradient norm statistics:\")\n",
    "for key, value in clipper.get_statistics().items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdamW vs Other Optimizers Comparison\n",
    "**Theoretical Analysis**:\n",
    "- **AdamW**: Decoupled weight decay, better regularization than L2 penalty\n",
    "- **Adam**: Adaptive learning rates with momentum, but couples weight decay with gradients\n",
    "- **SGD**: Simple but may require manual learning rate tuning\n",
    "- **RMSprop**: Good for RNNs, but Adam generally better for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerFactory:\n",
    "    \"\"\"Factory for creating different optimizers with consistent configurations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_adamw(model, lr=2e-4, weight_decay=0.01, betas=(0.9, 0.999), eps=1e-8):\n",
    "        \"\"\"\n",
    "        Create AdamW optimizer\n",
    "        \n",
    "        Theory: AdamW decouples weight decay from gradient updates:\n",
    "        θ_t = θ_{t-1} - α * (m_t / (√v_t + ε) + λ * θ_{t-1})\n",
    "        where λ is weight decay coefficient\n",
    "        \"\"\"\n",
    "        return optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=betas,\n",
    "            eps=eps\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_adam(model, lr=2e-4, weight_decay=0.01, betas=(0.9, 0.999), eps=1e-8):\n",
    "        \"\"\"\n",
    "        Create Adam optimizer with L2 regularization\n",
    "        \n",
    "        Theory: Adam with L2 penalty couples weight decay with gradients:\n",
    "        g_t = ∇f(θ_{t-1}) + λ * θ_{t-1}\n",
    "        Can interfere with adaptive learning rate computation\n",
    "        \"\"\"\n",
    "        return optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=betas,\n",
    "            eps=eps\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sgd(model, lr=0.01, weight_decay=0.01, momentum=0.9, nesterov=True):\n",
    "        \"\"\"\n",
    "        Create SGD with momentum and Nesterov acceleration\n",
    "        \n",
    "        Theory: Nesterov momentum looks ahead:\n",
    "        v_t = μ * v_{t-1} + g_t\n",
    "        θ_t = θ_{t-1} - α * (μ * v_t + g_t)\n",
    "        \"\"\"\n",
    "        return optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rmsprop(model, lr=2e-4, weight_decay=0.01, alpha=0.99, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Create RMSprop optimizer\n",
    "        \n",
    "        Theory: Uses moving average of squared gradients:\n",
    "        v_t = α * v_{t-1} + (1-α) * g_t²\n",
    "        θ_t = θ_{t-1} - α * g_t / (√v_t + ε)\n",
    "        \"\"\"\n",
    "        return optim.RMSprop(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            alpha=alpha,\n",
    "            eps=eps\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_radam(model, lr=2e-4, weight_decay=0.01, betas=(0.9, 0.999), eps=1e-8):\n",
    "        \"\"\"\n",
    "        Create RAdam (Rectified Adam) optimizer\n",
    "        \n",
    "        Theory: Provides variance rectification in early training stages\n",
    "        Switches between SGD and Adam based on variance estimate\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # RAdam might not be available in all PyTorch versions\n",
    "            from torch.optim import RAdam\n",
    "            return RAdam(\n",
    "                model.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay,\n",
    "                betas=betas,\n",
    "                eps=eps\n",
    "            )\n",
    "        except ImportError:\n",
    "            print(\"RAdam not available, falling back to AdamW\")\n",
    "            return OptimizerFactory.create_adamw(model, lr, weight_decay, betas, eps)\n",
    "\n",
    "# Demonstrate optimizer differences\n",
    "def analyze_optimizer_behavior():\n",
    "    \"\"\"Analyze how different optimizers behave on a simple optimization problem\"\"\"\n",
    "    \n",
    "    # Create a simple quadratic function to optimize\n",
    "    # f(x, y) = (x-1)² + (y-2)²\n",
    "    # Minimum at (1, 2)\n",
    "    \n",
    "    class SimpleModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.params = nn.Parameter(torch.tensor([0.0, 0.0]))  # Start at origin\n",
    "        \n",
    "        def forward(self):\n",
    "            x, y = self.params\n",
    "            return (x - 1)**2 + (y - 2)**2\n",
    "    \n",
    "    optimizers_config = {\n",
    "        'AdamW': lambda model: OptimizerFactory.create_adamw(model, lr=0.1, weight_decay=0.01),\n",
    "        'Adam': lambda model: OptimizerFactory.create_adam(model, lr=0.1, weight_decay=0.01),\n",
    "        'SGD': lambda model: OptimizerFactory.create_sgd(model, lr=0.1, weight_decay=0.01),\n",
    "        'RMSprop': lambda model: OptimizerFactory.create_rmsprop(model, lr=0.1, weight_decay=0.01)\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (name, optimizer_fn) in enumerate(optimizers_config.items()):\n",
    "        model = SimpleModel()\n",
    "        optimizer = optimizer_fn(model)\n",
    "        \n",
    "        trajectory = [model.params.data.clone().numpy()]\n",
    "        losses = []\n",
    "        \n",
    "        for step in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            loss = model()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            trajectory.append(model.params.data.clone().numpy())\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        trajectory = np.array(trajectory)\n",
    "        \n",
    "        # Plot optimization trajectory\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        \n",
    "        # Create contour plot of the function\n",
    "        x = np.linspace(-1, 3, 100)\n",
    "        y = np.linspace(-1, 4, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = (X - 1)**2 + (Y - 2)**2\n",
    "        \n",
    "        plt.contour(X, Y, Z, levels=20, alpha=0.6)\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], 'ro-', markersize=3, linewidth=2, alpha=0.8)\n",
    "        plt.plot(1, 2, 'g*', markersize=15, label='Global Minimum')\n",
    "        plt.plot(trajectory[0, 0], trajectory[0, 1], 'bo', markersize=8, label='Start')\n",
    "        plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=8, label='End')\n",
    "        \n",
    "        plt.title(f'{name} Optimization Path')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        print(f\"{name}: Final params: {trajectory[-1]}, Final loss: {losses[-1]:.6f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Optimizer Behavior Analysis:\")\n",
    "analyze_optimizer_behavior()\n",
    "\n",
    "print(\"\\nTheoretical Comparison:\")\n",
    "print(\"AdamW: Decoupled weight decay, best for transformers\")\n",
    "print(\"Adam: Coupled weight decay, may interfere with adaptive rates\")\n",
    "print(\"SGD: Simple and robust, but needs careful LR tuning\")\n",
    "print(\"RMSprop: Good for RNNs, but generally outperformed by Adam variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Optimization Strategy Implementation\n",
    "**Integration**: Combining AdamW + Learning Rate Scheduling + Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationConfig:\n",
    "    \"\"\"Configuration for optimization strategy\"\"\"\n",
    "    \n",
    "    # Model architecture (from previous experiments)\n",
    "    d_model = 512\n",
    "    num_layers = 6  # Reduced for faster experiments\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    dropout = 0.1\n",
    "    max_seq_length = 128\n",
    "    vocab_size = 50265\n",
    "    num_classes = 2\n",
    "    \n",
    "    # Optimization settings\n",
    "    optimizer_type = 'adamw'  # 'adamw', 'adam', 'sgd', 'rmsprop'\n",
    "    learning_rate = 2e-4\n",
    "    weight_decay = 0.01\n",
    "    adam_betas = (0.9, 0.999)\n",
    "    adam_eps = 1e-8\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler_type = 'cosine'  # 'linear', 'cosine', 'polynomial', 'inverse_sqrt'\n",
    "    warmup_steps = 500\n",
    "    total_steps = 5000  # Will be computed from epochs and data\n",
    "    \n",
    "    # Gradient clipping\n",
    "    gradient_clipping = True\n",
    "    clip_type = 'norm'  # 'norm', 'value', 'adaptive_norm', 'percentile'\n",
    "    clip_value = 1.0\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 32\n",
    "    max_epochs = 5\n",
    "    \n",
    "    device = device\n",
    "\n",
    "config = OptimizationConfig()\n",
    "print(f\"Optimization Configuration:\")\n",
    "print(f\"  Optimizer: {config.optimizer_type}\")\n",
    "print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  Scheduler: {config.scheduler_type}\")\n",
    "print(f\"  Gradient Clipping: {config.gradient_clipping} ({config.clip_type})\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Minimal Transformer for Testing\n",
    "**Simplified Architecture**: Fast training for optimization experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified transformer for fast experimentation\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attention(x, x, x, key_padding_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class OptimizedBotDetector(nn.Module):\n",
    "    \"\"\"Optimized transformer for testing optimization strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.position_embedding = nn.Embedding(config.max_seq_length, config.d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SimpleTransformerBlock(config.d_model, config.num_heads, config.d_ff, config.dropout)\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.ln_final = nn.LayerNorm(config.d_model)\n",
    "        self.classifier = nn.Linear(config.d_model, config.num_classes)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(0, seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Convert attention mask for MultiheadAttention (True = ignore)\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, key_padding_mask)\n",
    "        \n",
    "        # Classification (use CLS token - first token)\n",
    "        x = self.ln_final(x[:, 0, :])\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test model creation\n",
    "model = OptimizedBotDetector(config).to(config.device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, 1000, (2, 32)).to(config.device)\n",
    "test_mask = torch.ones_like(test_input).to(config.device)\n",
    "test_mask[0, 20:] = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_input, test_mask)\n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"  Input shape: {test_input.shape}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Training Loop with Optimization Monitoring\n",
    "**Complete Integration**: AdamW + Scheduling + Clipping + Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTrainer:\n",
    "    \"\"\"Advanced trainer with comprehensive optimization monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Setup optimization components\n",
    "        self.setup_optimizer()\n",
    "        self.setup_scheduler()\n",
    "        self.setup_gradient_clipper()\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Monitoring\n",
    "        self.optimization_history = {\n",
    "            'learning_rates': [],\n",
    "            'gradient_norms': [],\n",
    "            'gradient_clips': [],\n",
    "            'parameter_norms': [],\n",
    "            'train_losses': [],\n",
    "            'val_losses': [],\n",
    "            'val_accuracies': []\n",
    "        }\n",
    "    \n",
    "    def setup_optimizer(self):\n",
    "        \"\"\"Setup optimizer based on configuration\"\"\"\n",
    "        if self.config.optimizer_type == 'adamw':\n",
    "            self.optimizer = OptimizerFactory.create_adamw(\n",
    "                self.model, \n",
    "                lr=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                betas=self.config.adam_betas,\n",
    "                eps=self.config.adam_eps\n",
    "            )\n",
    "        elif self.config.optimizer_type == 'adam':\n",
    "            self.optimizer = OptimizerFactory.create_adam(\n",
    "                self.model, \n",
    "                lr=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay\n",
    "            )\n",
    "        elif self.config.optimizer_type == 'sgd':\n",
    "            self.optimizer = OptimizerFactory.create_sgd(\n",
    "                self.model, \n",
    "                lr=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.config.optimizer_type}\")\n",
    "    \n",
    "    def setup_scheduler(self):\n",
    "        \"\"\"Setup learning rate scheduler\"\"\"\n",
    "        total_steps = len(self.train_loader) * self.config.max_epochs\n",
    "        self.config.total_steps = total_steps\n",
    "        \n",
    "        if self.config.scheduler_type == 'linear':\n",
    "            self.scheduler = AdvancedLRScheduler.get_linear_schedule_with_warmup(\n",
    "                self.optimizer, self.config.warmup_steps, total_steps\n",
    "            )\n",
    "        elif self.config.scheduler_type == 'cosine':\n",
    "            self.scheduler = AdvancedLRScheduler.get_cosine_schedule_with_warmup(\n",
    "                self.optimizer, self.config.warmup_steps, total_steps\n",
    "            )\n",
    "        elif self.config.scheduler_type == 'polynomial':\n",
    "            self.scheduler = AdvancedLRScheduler.get_polynomial_schedule_with_warmup(\n",
    "                self.optimizer, self.config.warmup_steps, total_steps, power=1.5\n",
    "            )\n",
    "        elif self.config.scheduler_type == 'inverse_sqrt':\n",
    "            self.scheduler = AdvancedLRScheduler.get_inverse_sqrt_schedule_with_warmup(\n",
    "                self.optimizer, self.config.warmup_steps\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "    \n",
    "    def setup_gradient_clipper(self):\n",
    "        \"\"\"Setup gradient clipper\"\"\"\n",
    "        if self.config.gradient_clipping:\n",
    "            self.gradient_clipper = AdvancedGradientClipper(\n",
    "                clip_type=self.config.clip_type,\n",
    "                clip_value=self.config.clip_value\n",
    "            )\n",
    "        else:\n",
    "            self.gradient_clipper = None\n",
    "    \n",
    "    def compute_parameter_norm(self):\n",
    "        \"\"\"Compute total parameter norm\"\"\"\n",
    "        total_norm = 0.0\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad:\n",
    "                param_norm = param.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        return total_norm ** 0.5\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch with detailed monitoring\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Monitor gradients before clipping\n",
    "            grad_stats = {'original_norm': 0.0, 'was_clipped': False, 'clipped_norm': 0.0}\n",
    "            if self.gradient_clipper:\n",
    "                grad_stats = self.gradient_clipper.clip_gradients(self.model)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update learning rate\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Record monitoring data\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            param_norm = self.compute_parameter_norm()\n",
    "            \n",
    "            self.optimization_history['learning_rates'].append(current_lr)\n",
    "            self.optimization_history['gradient_norms'].append(grad_stats['original_norm'])\n",
    "            self.optimization_history['gradient_clips'].append(grad_stats['was_clipped'])\n",
    "            self.optimization_history['parameter_norms'].append(param_norm)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{current_lr:.2e}',\n",
    "                'grad_norm': f'{grad_stats[\"original_norm\"]:.3f}',\n",
    "                'clipped': grad_stats['was_clipped']\n",
    "            })\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Evaluating\", leave=False):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Complete training loop\"\"\"\n",
    "        print(f\"Starting training with optimization strategy:\")\n",
    "        print(f\"  Optimizer: {self.config.optimizer_type}\")\n",
    "        print(f\"  Scheduler: {self.config.scheduler_type}\")\n",
    "        print(f\"  Gradient Clipping: {self.config.gradient_clipping}\")\n",
    "        print(f\"  Total Steps: {self.config.total_steps}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy = self.evaluate()\n",
    "            \n",
    "            # Record epoch-level metrics\n",
    "            self.optimization_history['train_losses'].append(train_loss)\n",
    "            self.optimization_history['val_losses'].append(val_loss)\n",
    "            self.optimization_history['val_accuracies'].append(val_accuracy)\n",
    "            \n",
    "            # Print results\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}/{self.config.max_epochs}:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "            \n",
    "            # Show gradient clipping statistics\n",
    "            if self.gradient_clipper:\n",
    "                clip_rate = np.mean(self.optimization_history['gradient_clips'][-len(self.train_loader):])\n",
    "                print(f\"  Gradient Clip Rate: {clip_rate:.3f}\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def plot_optimization_history(self):\n",
    "        \"\"\"Plot comprehensive optimization monitoring\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Optimization Strategy Analysis', fontsize=16)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        axes[0, 0].plot(self.optimization_history['learning_rates'])\n",
    "        axes[0, 0].set_title('Learning Rate Schedule')\n",
    "        axes[0, 0].set_xlabel('Step')\n",
    "        axes[0, 0].set_ylabel('Learning Rate')\n",
    "        axes[0, 0].set_yscale('log')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient norms\n",
    "        axes[0, 1].plot(self.optimization_history['gradient_norms'], alpha=0.7)\n",
    "        if self.config.gradient_clipping:\n",
    "            axes[0, 1].axhline(y=self.config.clip_value, color='red', linestyle='--', \n",
    "                              label=f'Clip Threshold ({self.config.clip_value})')\n",
    "        axes[0, 1].set_title('Gradient Norms')\n",
    "        axes[0, 1].set_xlabel('Step')\n",
    "        axes[0, 1].set_ylabel('Gradient Norm')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Parameter norms\n",
    "        axes[0, 2].plot(self.optimization_history['parameter_norms'])\n",
    "        axes[0, 2].set_title('Parameter Norms')\n",
    "        axes[0, 2].set_xlabel('Step')\n",
    "        axes[0, 2].set_ylabel('Parameter Norm')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training loss\n",
    "        axes[1, 0].plot(self.optimization_history['train_losses'], label='Train', color='blue')\n",
    "        axes[1, 0].plot(self.optimization_history['val_losses'], label='Validation', color='red')\n",
    "        axes[1, 0].set_title('Loss Curves')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation accuracy\n",
    "        axes[1, 1].plot(self.optimization_history['val_accuracies'], color='green')\n",
    "        axes[1, 1].set_title('Validation Accuracy')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient clipping rate\n",
    "        if self.optimization_history['gradient_clips']:\n",
    "            # Calculate clipping rate over windows\n",
    "            window_size = max(1, len(self.train_loader) // 10)\n",
    "            clipping_rates = []\n",
    "            for i in range(0, len(self.optimization_history['gradient_clips']), window_size):\n",
    "                window = self.optimization_history['gradient_clips'][i:i+window_size]\n",
    "                clipping_rates.append(np.mean(window))\n",
    "            \n",
    "            axes[1, 2].plot(clipping_rates, color='orange')\n",
    "            axes[1, 2].set_title('Gradient Clipping Rate')\n",
    "            axes[1, 2].set_xlabel('Window')\n",
    "            axes[1, 2].set_ylabel('Clipping Rate')\n",
    "            axes[1, 2].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, 'No Gradient Clipping', \n",
    "                           ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Advanced optimization trainer ready!\")\n",
    "print(f\"Configuration loaded with {config.optimizer_type} optimizer and {config.scheduler_type} scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Synthetic Dataset for Optimization Testing\n",
    "**Fast Testing**: Generate synthetic data to test optimization strategies quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset for optimization experiments\n",
    "class OptimizationTestDataset(Dataset):\n",
    "    \"\"\"Synthetic dataset for testing optimization strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=2000, seq_length=128, vocab_size=1000):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Generate synthetic data with clear patterns\n",
    "        self.data = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Create patterns that are learnable but not trivial\n",
    "            if i < num_samples // 2:\n",
    "                # Class 0: sequences with smaller token IDs\n",
    "                seq = torch.randint(1, vocab_size // 2, (seq_length,))\n",
    "                label = 0\n",
    "            else:\n",
    "                # Class 1: sequences with larger token IDs\n",
    "                seq = torch.randint(vocab_size // 2, vocab_size, (seq_length,))\n",
    "                label = 1\n",
    "            \n",
    "            # Add some noise to make it more realistic\n",
    "            if torch.rand(1).item() < 0.1:  # 10% noise\n",
    "                seq = torch.randint(1, vocab_size, (seq_length,))\n",
    "            \n",
    "            # Create attention mask (simulate variable length sequences)\n",
    "            actual_length = torch.randint(seq_length // 2, seq_length + 1, (1,)).item()\n",
    "            attention_mask = torch.zeros(seq_length)\n",
    "            attention_mask[:actual_length] = 1\n",
    "            \n",
    "            self.data.append({\n",
    "                'input_ids': seq,\n",
    "                'attention_mask': attention_mask.long(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create datasets and data loaders\n",
    "full_dataset = OptimizationTestDataset(\n",
    "    num_samples=2000, \n",
    "    seq_length=config.max_seq_length, \n",
    "    vocab_size=5000  # Smaller vocab for faster training\n",
    ")\n",
    "\n",
    "# Update config for smaller vocab\n",
    "config.vocab_size = 5000\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dataset created:\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  Sequence length: {config.max_seq_length}\")\n",
    "\n",
    "# Test data loading\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  Input IDs shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  Attention mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"  Labels shape: {sample_batch['labels'].shape}\")\n",
    "print(f\"  Label distribution: {torch.bincount(sample_batch['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimization Strategy Experiments\n",
    "**Comparative Analysis**: Test different optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization experiments\n",
    "optimization_experiments = [\n",
    "    {\n",
    "        'name': 'AdamW_Cosine_Clipped',\n",
    "        'optimizer_type': 'adamw',\n",
    "        'scheduler_type': 'cosine',\n",
    "        'gradient_clipping': True,\n",
    "        'clip_type': 'norm',\n",
    "        'clip_value': 1.0,\n",
    "        'learning_rate': 2e-4\n",
    "    },\n",
    "    {\n",
    "        'name': 'AdamW_Linear_Clipped',\n",
    "        'optimizer_type': 'adamw',\n",
    "        'scheduler_type': 'linear',\n",
    "        'gradient_clipping': True,\n",
    "        'clip_type': 'norm',\n",
    "        'clip_value': 1.0,\n",
    "        'learning_rate': 2e-4\n",
    "    },\n",
    "    {\n",
    "        'name': 'Adam_Cosine_Clipped',\n",
    "        'optimizer_type': 'adam',\n",
    "        'scheduler_type': 'cosine',\n",
    "        'gradient_clipping': True,\n",
    "        'clip_type': 'norm',\n",
    "        'clip_value': 1.0,\n",
    "        'learning_rate': 2e-4\n",
    "    },\n",
    "    {\n",
    "        'name': 'AdamW_No_Clipping',\n",
    "        'optimizer_type': 'adamw',\n",
    "        'scheduler_type': 'cosine',\n",
    "        'gradient_clipping': False,\n",
    "        'learning_rate': 2e-4\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "experiment_results = []\n",
    "\n",
    "for exp in optimization_experiments:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RUNNING EXPERIMENT: {exp['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create experiment configuration\n",
    "    exp_config = OptimizationConfig()\n",
    "    exp_config.vocab_size = 5000  # Updated vocab size\n",
    "    \n",
    "    # Apply experiment settings\n",
    "    for key, value in exp.items():\n",
    "        if key != 'name' and hasattr(exp_config, key):\n",
    "            setattr(exp_config, key, value)\n",
    "    \n",
    "    try:\n",
    "        # Create fresh model for each experiment\n",
    "        model = OptimizedBotDetector(exp_config).to(exp_config.device)\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = OptimizationTrainer(model, train_loader, val_loader, exp_config)\n",
    "        \n",
    "        # Train model\n",
    "        trained_model = trainer.train()\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_val_loss, final_val_accuracy = trainer.evaluate()\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'name': exp['name'],\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'final_val_accuracy': final_val_accuracy,\n",
    "            'optimizer': exp['optimizer_type'],\n",
    "            'scheduler': exp['scheduler_type'],\n",
    "            'gradient_clipping': exp.get('gradient_clipping', False),\n",
    "            'learning_rate': exp['learning_rate']\n",
    "        }\n",
    "        \n",
    "        # Add optimization statistics\n",
    "        if trainer.gradient_clipper:\n",
    "            grad_stats = trainer.gradient_clipper.get_statistics()\n",
    "            result.update({\n",
    "                'mean_grad_norm': grad_stats.get('mean_norm', 0),\n",
    "                'max_grad_norm': grad_stats.get('max_norm', 0),\n",
    "                'clip_rate': np.mean(trainer.optimization_history['gradient_clips'])\n",
    "            })\n",
    "        \n",
    "        experiment_results.append(result)\n",
    "        \n",
    "        print(f\"\\nEXPERIMENT {exp['name']} COMPLETED:\")\n",
    "        print(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Final Validation Accuracy: {final_val_accuracy:.4f}\")\n",
    "        \n",
    "        # Plot optimization history for this experiment\n",
    "        trainer.plot_optimization_history()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in experiment {exp['name']}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        experiment_results.append({\n",
    "            'name': exp['name'],\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# Compare results\n",
    "if experiment_results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OPTIMIZATION STRATEGY COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    successful_results = [r for r in experiment_results if 'error' not in r]\n",
    "    \n",
    "    if successful_results:\n",
    "        results_df = pd.DataFrame(successful_results)\n",
    "        \n",
    "        # Display results\n",
    "        display_cols = ['name', 'final_val_accuracy', 'final_val_loss', 'optimizer', 'scheduler', 'gradient_clipping']\n",
    "        if 'clip_rate' in results_df.columns:\n",
    "            display_cols.append('clip_rate')\n",
    "        \n",
    "        print(results_df[display_cols].to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Find best performing strategy\n",
    "        best_accuracy_idx = results_df['final_val_accuracy'].idxmax()\n",
    "        best_result = results_df.loc[best_accuracy_idx]\n",
    "        \n",
    "        print(f\"\\nBEST PERFORMING STRATEGY: {best_result['name']}\")\n",
    "        print(f\"  Validation Accuracy: {best_result['final_val_accuracy']:.4f}\")\n",
    "        print(f\"  Validation Loss: {best_result['final_val_loss']:.4f}\")\n",
    "        print(f\"  Optimizer: {best_result['optimizer']}\")\n",
    "        print(f\"  Scheduler: {best_result['scheduler']}\")\n",
    "        print(f\"  Gradient Clipping: {best_result['gradient_clipping']}\")\n",
    "        \n",
    "        # Theoretical analysis\n",
    "        print(f\"\\nTHEORETICAL ANALYSIS:\")\n",
    "        adamw_results = results_df[results_df['optimizer'] == 'adamw']\n",
    "        if len(adamw_results) > 0:\n",
    "            adamw_avg_acc = adamw_results['final_val_accuracy'].mean()\n",
    "            print(f\"  AdamW average accuracy: {adamw_avg_acc:.4f}\")\n",
    "        \n",
    "        adam_results = results_df[results_df['optimizer'] == 'adam']\n",
    "        if len(adam_results) > 0:\n",
    "            adam_avg_acc = adam_results['final_val_accuracy'].mean()\n",
    "            print(f\"  Adam average accuracy: {adam_avg_acc:.4f}\")\n",
    "            \n",
    "            if len(adamw_results) > 0:\n",
    "                improvement = adamw_avg_acc - adam_avg_acc\n",
    "                print(f\"  AdamW improvement over Adam: {improvement:.4f} ({improvement/adam_avg_acc*100:+.1f}%)\")\n",
    "        \n",
    "        clipped_results = results_df[results_df['gradient_clipping'] == True]\n",
    "        unclipped_results = results_df[results_df['gradient_clipping'] == False]\n",
    "        \n",
    "        if len(clipped_results) > 0 and len(unclipped_results) > 0:\n",
    "            clipped_avg = clipped_results['final_val_accuracy'].mean()\n",
    "            unclipped_avg = unclipped_results['final_val_accuracy'].mean()\n",
    "            clip_improvement = clipped_avg - unclipped_avg\n",
    "            print(f\"  Gradient clipping improvement: {clip_improvement:.4f} ({clip_improvement/unclipped_avg*100:+.1f}%)\")\n",
    "    \n",
    "    # Show any failed experiments\n",
    "    failed_results = [r for r in experiment_results if 'error' in r]\n",
    "    if failed_results:\n",
    "        print(f\"\\nFAILED EXPERIMENTS:\")\n",
    "        for result in failed_results:\n",
    "            print(f\"  {result['name']}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Theoretical Analysis and Conclusions\n",
    "**Summary**: Key insights from optimization strategy experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXPERIMENT 5: GRADIENT DESCENT OPTIMIZATION STRATEGY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. ADAMW THEORETICAL ADVANTAGES:\")\n",
    "print(\"   ✓ Decoupled Weight Decay: Prevents regularization from interfering with\")\n",
    "print(\"     adaptive learning rate computation\")\n",
    "print(\"   ✓ Better Generalization: Weight decay applied directly to parameters,\")\n",
    "print(\"     not through gradients\")\n",
    "print(\"   ✓ Transformer Optimization: Specifically designed for modern deep learning\")\n",
    "\n",
    "print(\"\\n2. LEARNING RATE SCHEDULING BENEFITS:\")\n",
    "print(\"   ✓ Warmup Period: Allows gradients to stabilize before full optimization\")\n",
    "print(\"   ✓ Cosine Annealing: Smooth decay helps escape local minima\")\n",
    "print(\"   ✓ Linear Decay: Simple and effective for most transformer scenarios\")\n",
    "print(\"   ✓ Inverse Square Root: Original transformer paper schedule\")\n",
    "\n",
    "print(\"\\n3. GRADIENT CLIPPING IMPORTANCE:\")\n",
    "print(\"   ✓ Exploding Gradient Prevention: Critical for deep networks with attention\")\n",
    "print(\"   ✓ Training Stability: Prevents parameter updates from becoming too large\")\n",
    "print(\"   ✓ Convergence Reliability: Ensures stable training across different\")\n",
    "print(\"     sequence lengths\")\n",
    "\n",
    "print(\"\\n4. MATHEMATICAL FORMULATIONS:\")\n",
    "print(\"   AdamW Update Rule:\")\n",
    "print(\"   θ_t = θ_{t-1} - α * (m_t / (√v_t + ε) + λ * θ_{t-1})\")\n",
    "print(\"   where λ is weight decay coefficient (decoupled)\")\n",
    "print()\n",
    "print(\"   Gradient Clipping:\")\n",
    "print(\"   g_clipped = g * min(1, clip_value / ||g||_2)\")\n",
    "print(\"   Preserves gradient direction while limiting magnitude\")\n",
    "print()\n",
    "print(\"   Cosine Annealing:\")\n",
    "print(\"   lr_t = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(π * t / T))\")\n",
    "\n",
    "print(\"\\n5. IMPLEMENTATION INSIGHTS:\")\n",
    "print(\"   • Warmup steps should be ~10% of total training steps\")\n",
    "print(\"   • Gradient clipping threshold of 1.0 works well for most transformers\")\n",
    "print(\"   • Weight decay of 0.01 is a good starting point\")\n",
    "print(\"   • Learning rate 2e-4 is optimal for medium-sized transformers\")\n",
    "print(\"   • AdamW betas (0.9, 0.999) are well-tuned defaults\")\n",
    "\n",
    "if experiment_results and any('error' not in r for r in experiment_results):\n",
    "    successful_results = [r for r in experiment_results if 'error' not in r]\n",
    "    if successful_results:\n",
    "        print(\"\\n6. EXPERIMENTAL FINDINGS:\")\n",
    "        best_result = max(successful_results, key=lambda x: x['final_val_accuracy'])\n",
    "        print(f\"   • Best Strategy: {best_result['name']}\")\n",
    "        print(f\"   • Peak Accuracy: {best_result['final_val_accuracy']:.4f}\")\n",
    "        print(f\"   • Configuration: {best_result['optimizer']} + {best_result['scheduler']} + clipping={best_result['gradient_clipping']}\")\n",
    "        \n",
    "        # Calculate improvement statistics\n",
    "        accuracies = [r['final_val_accuracy'] for r in successful_results]\n",
    "        accuracy_range = max(accuracies) - min(accuracies)\n",
    "        print(f\"   • Performance Range: {accuracy_range:.4f} ({accuracy_range/min(accuracies)*100:.1f}% relative)\")\n",
    "        print(f\"   • Average Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "\n",
    "print(\"\\n7. PRODUCTION RECOMMENDATIONS:\")\n",
    "print(\"   1. Use AdamW optimizer with decoupled weight decay\")\n",
    "print(\"   2. Implement cosine annealing with warmup for LR scheduling\")\n",
    "print(\"   3. Apply gradient norm clipping with threshold 1.0\")\n",
    "print(\"   4. Monitor gradient norms and clipping rates during training\")\n",
    "print(\"   5. Start with lr=2e-4, weight_decay=0.01, warmup=10% of steps\")\n",
    "\n",
    "print(\"\\n8. NEXT STEPS FOR ADVANCED OPTIMIZATION:\")\n",
    "print(\"   • Experiment with adaptive clipping thresholds\")\n",
    "print(\"   • Try different warmup schedules (linear vs quadratic)\")\n",
    "print(\"   • Implement layerwise learning rate decay\")\n",
    "print(\"   • Test advanced optimizers (Lion, Sophia) when available\")\n",
    "print(\"   • Add gradient noise for better generalization\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPERIMENT 5 COMPLETED - OPTIMIZATION STRATEGY ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n",
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.5"\n",
  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}