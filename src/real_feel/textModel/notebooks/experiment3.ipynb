{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Cresci-2017 Data Loading & Neural Network Architecture\n",
    "Integration of real dataset with optimized transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmartinez/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cresci-2017 Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRESCI-2017 DATASET BREAKDOWN\n",
      "==================================================\n",
      "Total accounts: 14,398\n",
      "Total tweets: 18,179,186\n",
      "\n",
      "Class distribution:\n",
      "  genuine_accounts      :  3474 acc (24.1%) |  8377522 tweets (46.1%)\n",
      "  social_spambots_1     :   991 acc ( 6.9%) |  1610176 tweets ( 8.9%)\n",
      "  social_spambots_2     :  3457 acc (24.0%) |   428542 tweets ( 2.4%)\n",
      "  social_spambots_3     :   464 acc ( 3.2%) |  1418626 tweets ( 7.8%)\n",
      "  traditional_spambots_1:  1000 acc ( 6.9%) |   145094 tweets ( 0.8%)\n",
      "  traditional_spambots_2:   100 acc ( 0.7%) |    74957 tweets ( 0.4%)\n",
      "  traditional_spambots_3:   433 acc ( 3.0%) |  5794931 tweets (31.9%)\n",
      "  traditional_spambots_4:  1128 acc ( 7.8%) |   133311 tweets ( 0.7%)\n",
      "  fake_followers        :  3351 acc (23.3%) |   196027 tweets ( 1.1%)\n",
      "\n",
      "Class imbalance issues:\n",
      "  Largest: genuine_accounts (8.3M tweets, 47.1%)\n",
      "  Smallest: traditional_spambots_2 (75K tweets, 0.4%)\n",
      "  Imbalance ratio: 111.8:1\n"
     ]
    }
   ],
   "source": [
    "# Dataset metadata from the paper\n",
    "CRESCI_DATASETS = {\n",
    "    'genuine_accounts': {'accounts': 3474, 'tweets': 8377522, 'year': 2011, 'label': 0},\n",
    "    'social_spambots_1': {'accounts': 991, 'tweets': 1610176, 'year': 2012, 'label': 1},\n",
    "    'social_spambots_2': {'accounts': 3457, 'tweets': 428542, 'year': 2014, 'label': 2}, \n",
    "    'social_spambots_3': {'accounts': 464, 'tweets': 1418626, 'year': 2011, 'label': 3},\n",
    "    'traditional_spambots_1': {'accounts': 1000, 'tweets': 145094, 'year': 2009, 'label': 4},\n",
    "    'traditional_spambots_2': {'accounts': 100, 'tweets': 74957, 'year': 2014, 'label': 5},\n",
    "    'traditional_spambots_3': {'accounts': 433, 'tweets': 5794931, 'year': 2013, 'label': 6},\n",
    "    'traditional_spambots_4': {'accounts': 1128, 'tweets': 133311, 'year': 2009, 'label': 7},\n",
    "    'fake_followers': {'accounts': 3351, 'tweets': 196027, 'year': 2012, 'label': 8}\n",
    "}\n",
    "\n",
    "# Calculate distribution\n",
    "total_accounts = sum(d['accounts'] for d in CRESCI_DATASETS.values())\n",
    "total_tweets = sum(d['tweets'] for d in CRESCI_DATASETS.values())\n",
    "\n",
    "print(\"CRESCI-2017 DATASET BREAKDOWN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total accounts: {total_accounts:,}\")\n",
    "print(f\"Total tweets: {total_tweets:,}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "\n",
    "for name, data in CRESCI_DATASETS.items():\n",
    "    acc_pct = data['accounts'] / total_accounts * 100\n",
    "    tweet_pct = data['tweets'] / total_tweets * 100\n",
    "    print(f\"  {name:<22}: {data['accounts']:>5} acc ({acc_pct:>4.1f}%) | {data['tweets']:>8} tweets ({tweet_pct:>4.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass imbalance issues:\")\n",
    "print(f\"  Largest: genuine_accounts (8.3M tweets, 47.1%)\")\n",
    "print(f\"  Smallest: traditional_spambots_2 (75K tweets, 0.4%)\")\n",
    "print(f\"  Imbalance ratio: {8377522/74957:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategic Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CresciDataLoader:\n",
    "    def __init__(self, data_root=\"../datasets/datasets_full.csv/\"):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.datasets = CRESCI_DATASETS\n",
    "        \n",
    "    def load_dataset_tweets(self, dataset_name, sample_size=None):\n",
    "        \"\"\"Load tweets from a specific dataset with optional sampling\"\"\"\n",
    "        tweet_path = self.data_root / f\"{dataset_name}.csv\" / \"tweets.csv\"\n",
    "        \n",
    "        if not tweet_path.exists():\n",
    "            print(f\"Warning: {tweet_path} not found\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(tweet_path)\n",
    "            \n",
    "            # Sample if specified (for managing class imbalance)\n",
    "            if sample_size and len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "            # Add metadata\n",
    "            df['dataset'] = dataset_name\n",
    "            df['binary_label'] = 0 if dataset_name == 'genuine_accounts' else 1  # Human=0, Bot=1\n",
    "            df['multiclass_label'] = self.datasets[dataset_name]['label']\n",
    "            df['year'] = self.datasets[dataset_name]['year']\n",
    "            \n",
    "            print(f\"Loaded {len(df):,} tweets from {dataset_name}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {dataset_name}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def create_balanced_dataset(self, strategy='undersample', max_per_class=50000):\n",
    "        \"\"\"Create balanced dataset using different strategies\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        print(f\"\\nCreating balanced dataset with strategy: {strategy}\")\n",
    "        print(f\"Max samples per class: {max_per_class:,}\")\n",
    "        \n",
    "        for dataset_name in self.datasets.keys():\n",
    "            # Load with sampling to balance classes\n",
    "            if strategy == 'undersample':\n",
    "                sample_size = min(max_per_class, self.datasets[dataset_name]['tweets'])\n",
    "            else:  # 'full'\n",
    "                sample_size = None\n",
    "                \n",
    "            df = self.load_dataset_tweets(dataset_name, sample_size)\n",
    "            \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data loaded!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine all datasets\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Clean text column\n",
    "        if 'text' not in combined_df.columns:\n",
    "            print(\"Available columns:\", combined_df.columns.tolist())\n",
    "            return combined_df\n",
    "        \n",
    "        # Remove empty tweets\n",
    "        combined_df = combined_df.dropna(subset=['text'])\n",
    "        combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "        \n",
    "        print(f\"\\nFinal dataset: {len(combined_df):,} tweets\")\n",
    "        print(f\"Binary distribution:\")\n",
    "        print(combined_df['binary_label'].value_counts().sort_index())\n",
    "        print(f\"\\nMulticlass distribution:\")\n",
    "        print(combined_df.groupby(['dataset', 'multiclass_label']).size())\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = CresciDataLoader()\n",
    "\n",
    "# Create balanced dataset (start small for testing)\n",
    "cresci_df = data_loader.create_balanced_dataset(strategy='undersample', max_per_class=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalBotConfig:\n",
    "    # Architecture (from our optimization)\n",
    "    d_model = 512\n",
    "    num_layers = 9\n",
    "    num_heads = 12\n",
    "    d_ff = 2048\n",
    "    dropout = 0.15\n",
    "    max_seq_length = 128\n",
    "    \n",
    "    # Tasks\n",
    "    num_binary_classes = 2      # Human vs Bot\n",
    "    num_multiclass_classes = 9  # All categories\n",
    "    \n",
    "    # Training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = OptimalBotConfig()\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.qkv_projection = nn.Linear(d_model, 3 * d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Project and reshape for multi-head attention\n",
    "        qkv = self.qkv_projection(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        \n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        attended_values = torch.matmul(attention_weights, v)\n",
    "        attended_values = attended_values.permute(0, 2, 1, 3).contiguous()\n",
    "        attended_values = attended_values.reshape(batch_size, seq_len, d_model)\n",
    "        \n",
    "        output = self.output_projection(attended_values)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.self_attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(f\"Architecture components defined for {config.d_model}d model with {config.num_heads} heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Bot Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedBotDetector(nn.Module):\n",
    "    def __init__(self, config, vocab_size=50265):  # RoBERTa vocab size\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings (we'll replace with pre-trained later)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, config.d_model)\n",
    "        self.position_embedding = nn.Embedding(config.max_seq_length, config.d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                config.d_model, \n",
    "                config.num_heads, \n",
    "                config.d_ff, \n",
    "                config.dropout\n",
    "            ) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification heads\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Binary classification (Human vs Bot)\n",
    "        self.binary_classifier = nn.Linear(config.d_model, config.num_binary_classes)\n",
    "        \n",
    "        # Multiclass classification (Bot types) - for future use\n",
    "        self.multiclass_classifier = nn.Linear(config.d_model, config.num_multiclass_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, task='binary'):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        \n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = embeddings * math.sqrt(self.config.d_model)  # Scale\n",
    "        \n",
    "        # Create attention mask for padding\n",
    "        if attention_mask is not None:\n",
    "            # Convert to format expected by attention layers\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -1e9\n",
    "        else:\n",
    "            extended_mask = None\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        hidden_states = embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states, extended_mask)\n",
    "        \n",
    "        # Get [CLS] token representation (first token)\n",
    "        cls_representation = hidden_states[:, 0, :]\n",
    "        cls_representation = self.layer_norm(cls_representation)\n",
    "        cls_representation = self.dropout(cls_representation)\n",
    "        \n",
    "        # Classification based on task\n",
    "        if task == 'binary':\n",
    "            logits = self.binary_classifier(cls_representation)\n",
    "        elif task == 'multiclass':\n",
    "            logits = self.multiclass_classifier(cls_representation)\n",
    "        else:\n",
    "            # Return both for multi-task training (future)\n",
    "            return {\n",
    "                'binary_logits': self.binary_classifier(cls_representation),\n",
    "                'multiclass_logits': self.multiclass_classifier(cls_representation)\n",
    "            }\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params\n",
    "\n",
    "# Create model\n",
    "model = OptimizedBotDetector(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "total_params, trainable_params = model.get_num_parameters()\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Parameters: {total_params:,} total ({trainable_params:,} trainable)\")\n",
    "print(f\"  Architecture: {config.d_model}d × {config.num_layers}L × {config.num_heads}H\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, 1000, (2, config.max_seq_length), device=config.device)\n",
    "test_mask = torch.ones_like(test_input, device=config.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_input, test_mask, task='binary')\n",
    "    print(f\"\\nTest forward pass: {test_input.shape} → {test_output.shape}\")\n",
    "    print(f\"Output range: [{test_output.min().item():.3f}, {test_output.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Subword Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Twitter-RoBERTa tokenizer from experiment2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "\n",
    "class CresciDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, task='binary'):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.task = task\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        if self.task == 'binary':\n",
    "            label = self.labels['binary'][idx]\n",
    "        else:\n",
    "            label = self.labels['multiclass'][idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Test with real data if available\n",
    "if not cresci_df.empty and 'text' in cresci_df.columns:\n",
    "    sample_texts = cresci_df['text'].head(100).tolist()\n",
    "    sample_labels = {\n",
    "        'binary': cresci_df['binary_label'].head(100).tolist(),\n",
    "        'multiclass': cresci_df['multiclass_label'].head(100).tolist()\n",
    "    }\n",
    "    \n",
    "    # Create dataset\n",
    "    test_dataset = CresciDataset(sample_texts, sample_labels, tokenizer, task='binary')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    # Test batch\n",
    "    sample_batch = next(iter(test_dataloader))\n",
    "    print(f\"\\nReal data batch test:\")\n",
    "    print(f\"  Input shape: {sample_batch['input_ids'].shape}\")\n",
    "    print(f\"  Labels: {sample_batch['labels'][:5].tolist()}\")\n",
    "    \n",
    "    # Test model prediction\n",
    "    with torch.no_grad():\n",
    "        batch_input = sample_batch['input_ids'].to(config.device)\n",
    "        batch_mask = sample_batch['attention_mask'].to(config.device)\n",
    "        predictions = model(batch_input, batch_mask, task='binary')\n",
    "        probabilities = F.softmax(predictions, dim=1)\n",
    "        \n",
    "    print(f\"  Predictions shape: {predictions.shape}\")\n",
    "    print(f\"  Sample probabilities: {probabilities[:3].cpu().numpy()}\")\n",
    "else:\n",
    "    print(\"\\nNo real data available - using synthetic test\")\n",
    "    # Synthetic test\n",
    "    test_texts = [\n",
    "        \"Just had amazing coffee this morning ☕\",\n",
    "        \"Follow for follow! F4F! #followback #follow\",\n",
    "        \"Beautiful weather today, perfect for a walk\", \n",
    "        \"URGENT! Free money! Click now! #scam #fake\"\n",
    "    ]\n",
    "    test_labels = {'binary': [0, 1, 0, 1], 'multiclass': [0, 2, 0, 5]}\n",
    "    \n",
    "    synthetic_dataset = CresciDataset(test_texts, test_labels, tokenizer)\n",
    "    synthetic_loader = DataLoader(synthetic_dataset, batch_size=2)\n",
    "    \n",
    "    sample_batch = next(iter(synthetic_loader))\n",
    "    print(f\"Synthetic batch test successful: {sample_batch['input_ids'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realFeel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
