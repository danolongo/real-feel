{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Subword Tokenization Implementation\n",
    "Focus on implementing robust Twitter-aware subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmartinez/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Model: 512d, 9L, 12H\n"
     ]
    }
   ],
   "source": [
    "class OptimalBotConfig:\n",
    "    d_model = 512           # Balanced performance/speed\n",
    "    num_layers = 9          # Captures bot patterns effectively\n",
    "    num_heads = 12          # Rich attention diversity\n",
    "    d_ff = 2048            # 4x d_model ratio\n",
    "    max_seq_length = 128   # Twitter-optimized\n",
    "    dropout = 0.15         # Higher for overfitting prevention\n",
    "    num_classes = 2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = OptimalBotConfig()\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Model: {config.d_model}d, {config.num_layers}L, {config.num_heads}H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter-Aware Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: cardiffnlp/twitter-roberta-base\n",
      "Vocabulary size: 50,265\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "source": [
    "class TwitterBotTokenizer:\n",
    "    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base\"):\n",
    "        \"\"\"Initialize with Twitter-aware RoBERTa tokenizer\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "        \n",
    "        print(f\"Loaded tokenizer: {model_name}\")\n",
    "        print(f\"Vocabulary size: {self.vocab_size:,}\")\n",
    "        print(f\"Special tokens: {self.tokenizer.special_tokens_map}\")\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Analyze tokenization of a text sample\"\"\"\n",
    "        # Raw tokenization\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        token_ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        \n",
    "        # With length constraints\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            max_length=config.max_seq_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'tokens': tokens,\n",
    "            'token_ids': token_ids,\n",
    "            'encoded_ids': encoded['input_ids'].squeeze().tolist(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze().tolist(),\n",
    "            'actual_length': len(token_ids),\n",
    "            'padded_length': len(encoded['input_ids'].squeeze())\n",
    "        }\n",
    "    \n",
    "    def batch_encode(self, texts, max_length=None):\n",
    "        \"\"\"Encode a batch of texts\"\"\"\n",
    "        max_length = max_length or config.max_seq_length\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'],\n",
    "            'attention_mask': encoded['attention_mask']\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "bot_tokenizer = TwitterBotTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Analysis on Bot vs Human Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOKENIZATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Text: Just had an amazing coffee at my local cafe â˜• Perfect start ...\n",
      "Tokens (16): ['Just', 'Ä had', 'Ä an', 'Ä amazing', 'Ä coffee', 'Ä at', 'Ä my', 'Ä local']...\n",
      "Actual length: 18 tokens\n",
      "Padded to: 128 tokens\n",
      "Efficiency: 14.1% (non-padding)\n",
      "\n",
      "Example 2:\n",
      "Text: Working from home today, feeling productive and grateful for...\n",
      "Tokens (11): ['Working', 'Ä from', 'Ä home', 'Ä today', ',', 'Ä feeling', 'Ä productive', 'Ä and']...\n",
      "Actual length: 13 tokens\n",
      "Padded to: 128 tokens\n",
      "Efficiency: 10.2% (non-padding)\n",
      "\n",
      "Example 3:\n",
      "Text: Follow me for amazing deals! #sponsored #ad #promotion #sale...\n",
      "Tokens (18): ['Follow', 'Ä me', 'Ä for', 'Ä amazing', 'Ä deals', '!', 'Ä #', 'sponsored']...\n",
      "Actual length: 20 tokens\n",
      "Padded to: 128 tokens\n",
      "Efficiency: 15.6% (non-padding)\n",
      "\n",
      "Example 4:\n",
      "Text: ðŸš¨ URGENT: Click here for FREE MONEY! Link in bio! #scam #fak...\n",
      "Tokens (26): ['Ã°Å', 'Ä¼', 'Â¨', 'Ä UR', 'G', 'ENT', ':', 'Ä Click']...\n",
      "Actual length: 28 tokens\n",
      "Padded to: 128 tokens\n",
      "Efficiency: 21.9% (non-padding)\n",
      "\n",
      "Example 5:\n",
      "Text: RT @user123: Buy now!!! Limited time offer!!! Don't miss out...\n",
      "Tokens (17): ['RT', 'Ä @', 'user', '123', ':', 'Ä Buy', 'Ä now', '!!!']...\n",
      "Actual length: 19 tokens\n",
      "Padded to: 128 tokens\n",
      "Efficiency: 14.8% (non-padding)\n",
      "\n",
      "Example 6:\n",
      "Text: @mention1 @mention2 @mention3 check this out http://bit.ly/s...\n",
      "Tokens (23): ['@', 'ment', 'ion', '1', 'Ä @', 'ment', 'ion', '2']...\n",
      "Actual length: 25 tokens\n",
      "Padded to: 128 tokens\n",
      "Efficiency: 19.5% (non-padding)\n"
     ]
    }
   ],
   "source": [
    "# Test examples from different categories\n",
    "test_examples = [\n",
    "    # Genuine human tweets\n",
    "    \"Just had an amazing coffee at my local cafe â˜• Perfect start to Monday!\",\n",
    "    \"Working from home today, feeling productive and grateful for flexibility\",\n",
    "    \n",
    "    # Bot-like patterns  \n",
    "    \"Follow me for amazing deals! #sponsored #ad #promotion #sale #discount\",\n",
    "    \"ðŸš¨ URGENT: Click here for FREE MONEY! Link in bio! #scam #fake #followers\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"RT @user123: Buy now!!! Limited time offer!!! Don't miss out!!!\",\n",
    "    \"@mention1 @mention2 @mention3 check this out http://bit.ly/suspicious\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOKENIZATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, text in enumerate(test_examples):\n",
    "    analysis = bot_tokenizer.analyze_text(text)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: {analysis['original_text'][:60]}{'...' if len(analysis['original_text']) > 60 else ''}\")\n",
    "    print(f\"Tokens ({len(analysis['tokens'])}): {analysis['tokens'][:8]}{'...' if len(analysis['tokens']) > 8 else ''}\")\n",
    "    print(f\"Actual length: {analysis['actual_length']} tokens\")\n",
    "    print(f\"Padded to: {analysis['padded_length']} tokens\")\n",
    "    print(f\"Efficiency: {analysis['actual_length']/analysis['padded_length']*100:.1f}% (non-padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary and Token Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKENIZATION EFFICIENCY ANALYSIS\n",
      "==================================================\n",
      "Sample size: 100 texts\n",
      "\n",
      "Length Statistics:\n",
      "  Mean length: 18.2 tokens\n",
      "  Median length: 17.0 tokens\n",
      "  95th percentile: 23.0 tokens\n",
      "  Max length: 23 tokens\n",
      "  Texts > 128 tokens: 0/100 (0.0%)\n",
      "\n",
      "OOV (Unknown) Tokens:\n",
      "  Mean OOV per text: 0.00\n",
      "  Texts with OOV: 0/100 (0.0%)\n",
      "\n",
      "Twitter-specific Elements:\n",
      "  Texts with mentions: 20/100 (20.0%)\n",
      "  Texts with urls: 20/100 (20.0%)\n",
      "  Texts with hashtags: 40/100 (40.0%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_tokenization_efficiency(texts, tokenizer):\n",
    "    \"\"\"Analyze how efficiently our tokenizer handles the text corpus\"\"\"\n",
    "    stats = {\n",
    "        'lengths': [],\n",
    "        'oov_counts': [],\n",
    "        'special_token_usage': {'mentions': 0, 'urls': 0, 'hashtags': 0}\n",
    "    }\n",
    "    \n",
    "    for text in texts[:100]:  # Sample for analysis\n",
    "        tokens = tokenizer.tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.tokenizer.encode(text, add_special_tokens=True)\n",
    "        \n",
    "        stats['lengths'].append(len(token_ids))\n",
    "        \n",
    "        # Count OOV (unknown) tokens\n",
    "        oov_count = sum(1 for tid in token_ids if tid == tokenizer.tokenizer.unk_token_id)\n",
    "        stats['oov_counts'].append(oov_count)\n",
    "        \n",
    "        # Check for Twitter-specific patterns\n",
    "        text_lower = text.lower()\n",
    "        if '@' in text: stats['special_token_usage']['mentions'] += 1\n",
    "        if 'http' in text: stats['special_token_usage']['urls'] += 1\n",
    "        if '#' in text: stats['special_token_usage']['hashtags'] += 1\n",
    "    \n",
    "    # Calculate statistics\n",
    "    lengths = np.array(stats['lengths'])\n",
    "    oov_counts = np.array(stats['oov_counts'])\n",
    "    \n",
    "    print(f\"\\nTOKENIZATION EFFICIENCY ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Sample size: {len(texts[:100])} texts\")\n",
    "    print(f\"\\nLength Statistics:\")\n",
    "    print(f\"  Mean length: {lengths.mean():.1f} tokens\")\n",
    "    print(f\"  Median length: {np.median(lengths):.1f} tokens\")\n",
    "    print(f\"  95th percentile: {np.percentile(lengths, 95):.1f} tokens\")\n",
    "    print(f\"  Max length: {lengths.max()} tokens\")\n",
    "    print(f\"  Texts > 128 tokens: {(lengths > 128).sum()}/{len(lengths)} ({(lengths > 128).mean()*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nOOV (Unknown) Tokens:\")\n",
    "    print(f\"  Mean OOV per text: {oov_counts.mean():.2f}\")\n",
    "    print(f\"  Texts with OOV: {(oov_counts > 0).sum()}/{len(oov_counts)} ({(oov_counts > 0).mean()*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTwitter-specific Elements:\")\n",
    "    for element, count in stats['special_token_usage'].items():\n",
    "        print(f\"  Texts with {element}: {count}/{len(texts[:100])} ({count/len(texts[:100])*100:.1f}%)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# For demo, create some sample texts\n",
    "sample_texts = [\n",
    "    \"Follow for follow! #F4F #follow #followback\",\n",
    "    \"Just finished a great workout! Feeling energized ðŸ’ª\",\n",
    "    \"RT @sponsor: AMAZING DEAL! Click now! http://bit.ly/deal\",\n",
    "    \"Coffee with friends this morning â˜• Perfect way to start the day\",\n",
    "    \"ðŸš¨ URGENT: Free money! DM me now! #money #free #cash\"\n",
    "] * 20  # Repeat to get 100 samples\n",
    "\n",
    "efficiency_stats = analyze_tokenization_efficiency(sample_texts, bot_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample batch shapes:\n",
      "  Input IDs: torch.Size([2, 128])\n",
      "  Attention mask: torch.Size([2, 128])\n",
      "  Labels: torch.Size([2])\n",
      "\n",
      "First text tokens: [0, 21518, 2340, 3545, 59, 1650, 2, 1, 1, 1]\n",
      "First attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TwitterBotDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length or config.max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Use the optimized tokenizer\n",
    "        encoding = self.tokenizer.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Test dataset creation\n",
    "test_texts = [\n",
    "    \"Genuine human tweet about daily life\",\n",
    "    \"Follow me for deals! #sponsored #ad\",\n",
    "    \"Another normal tweet about weather\",\n",
    "    \"URGENT! Free money! Click link!\"\n",
    "]\n",
    "test_labels = [0, 1, 0, 1]  # 0=human, 1=bot\n",
    "\n",
    "dataset = TwitterBotDataset(test_texts, test_labels, bot_tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Test a batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Input IDs: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  Attention mask: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"  Labels: {sample_batch['labels'].shape}\")\n",
    "print(f\"\\nFirst text tokens: {sample_batch['input_ids'][0][:10].tolist()}\")\n",
    "print(f\"First attention mask: {sample_batch['attention_mask'][0][:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realFeel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
