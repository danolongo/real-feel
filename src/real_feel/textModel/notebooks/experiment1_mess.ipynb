{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c75421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd50cd",
   "metadata": {},
   "source": [
    "experiment1 implements a BERT encoder-only architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c443c0a2",
   "metadata": {},
   "source": [
    "below is the sample multi-head attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a6754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmartinez/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/transformers/__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     is_pretty_midi_available,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/transformers/utils/auto_docstring.py:33\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m PATH_TO_TRANSFORMERS = \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n\u001b[32m     46\u001b[39m PLACEHOLDER_TO_AUTO_MODULE = {\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage_processor_class\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33mimage_processing_auto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIMAGE_PROCESSOR_MAPPING_NAMES\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvideo_processor_class\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33mvideo_processing_auto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVIDEO_PROCESSOR_MAPPING_NAMES\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig_class\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33mconfiguration_auto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCONFIG_MAPPING_NAMES\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     52\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.3-macos-aarch64-none/lib/python3.12/pathlib.py:1240\u001b[39m, in \u001b[36mPath.resolve\u001b[39m\u001b[34m(self, strict)\u001b[39m\n\u001b[32m   1237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSymlink loop from \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % e.filename)\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flavour\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1242\u001b[39m     check_eloop(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:436\u001b[39m, in \u001b[36mrealpath\u001b[39m\u001b[34m(filename, strict)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:423\u001b[39m, in \u001b[36mabspath\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "# Transformer Bot Detection System - Cresci-2017 Dataset\n",
    "# Implementation of BERT-style encoder-only architecture for Twitter bot detection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80726a2",
   "metadata": {},
   "source": [
    "Attention(Q, K, V) = softmax((QK^T)/sqrt(d_K)) * V\n",
    "\n",
    "Q, K, V are queries, keys, values derived from the same source in self-attention.\n",
    "Results in valuesvalues: the weighted sum for each position and head.\n",
    "Softmax ensures the attention weights sum to 1.\n",
    "If masking, irrelevant positions (like future tokens or padding) get large negative values in logits, so after softmax attention there is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b869fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    # (batch, heads, seq_len, head_dim) @ (batch, heads, head_dim, seq_len) --> (batch, heads, seq_len, seq_len)\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    # (batch, heads, seq_len, seq_len) @ (batch, heads, seq_len, head_dim) --> (batch, heads, seq_len, head_dim)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c081450",
   "metadata": {},
   "source": [
    "Multi-Head Attention Class\n",
    "Every step mimics the original Transformer:\n",
    "\n",
    "Project to QKV,\n",
    "Reshape for multiple heads,\n",
    "Split into Q, K, V,\n",
    "Compute attention,\n",
    "Concatenate heads,\n",
    "Linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Multi-Head Attention (cleaned up for production use)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.qkv_projection = nn.Linear(d_model, 3 * d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv_projection(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # (batch, heads, seq_len, 3*head_dim)\n",
    "        \n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Each: (batch, heads, seq_len, head_dim)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attended_values = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended_values = attended_values.permute(0, 2, 1, 3).contiguous()\n",
    "        attended_values = attended_values.reshape(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.output_projection(attended_values)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7a4wuc49q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.self_attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vf3mhhd72ga",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tokenizer for Twitter Text\n",
    "class TwitterTokenizer:\n",
    "    def __init__(self, vocab_size=30000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = '[PAD]'\n",
    "        self.UNK_TOKEN = '[UNK]'\n",
    "        self.CLS_TOKEN = '[CLS]'\n",
    "        self.SEP_TOKEN = '[SEP]'\n",
    "        self.URL_TOKEN = '[URL]'\n",
    "        self.MENTION_TOKEN = '[MENTION]'\n",
    "        \n",
    "        self.special_tokens = [\n",
    "            self.PAD_TOKEN, self.UNK_TOKEN, self.CLS_TOKEN, \n",
    "            self.SEP_TOKEN, self.URL_TOKEN, self.MENTION_TOKEN\n",
    "        ]\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess Twitter text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "                     self.URL_TOKEN, text)\n",
    "        \n",
    "        # Replace mentions\n",
    "        text = re.sub(r'@\\w+', self.MENTION_TOKEN, text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple whitespace tokenization\"\"\"\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        return processed_text.split()\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        # Count words\n",
    "        for text in tqdm(texts):\n",
    "            tokens = self.tokenize(text)\n",
    "            self.word_counts.update(tokens)\n",
    "        \n",
    "        # Add special tokens first\n",
    "        for token in self.special_tokens:\n",
    "            self.word_to_idx[token] = len(self.word_to_idx)\n",
    "            self.idx_to_word[len(self.idx_to_word)] = token\n",
    "        \n",
    "        # Add most frequent words\n",
    "        most_common = self.word_counts.most_common(self.vocab_size - len(self.special_tokens))\n",
    "        for word, count in most_common:\n",
    "            if word not in self.word_to_idx:\n",
    "                self.word_to_idx[word] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = word\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word_to_idx)}\")\n",
    "        return self\n",
    "    \n",
    "    def encode(self, text, max_length=256):\n",
    "        \"\"\"Convert text to token ids\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # Add CLS token at beginning\n",
    "        token_ids = [self.word_to_idx[self.CLS_TOKEN]]\n",
    "        \n",
    "        # Add text tokens\n",
    "        for token in tokens[:max_length-2]:  # Reserve space for CLS and SEP\n",
    "            token_ids.append(self.word_to_idx.get(token, self.word_to_idx[self.UNK_TOKEN]))\n",
    "        \n",
    "        # Add SEP token\n",
    "        token_ids.append(self.word_to_idx[self.SEP_TOKEN])\n",
    "        \n",
    "        # Pad to max_length\n",
    "        while len(token_ids) < max_length:\n",
    "            token_ids.append(self.word_to_idx[self.PAD_TOKEN])\n",
    "            \n",
    "        return token_ids[:max_length]\n",
    "    \n",
    "    def create_attention_mask(self, token_ids):\n",
    "        \"\"\"Create attention mask (1 for real tokens, 0 for padding)\"\"\"\n",
    "        return [1 if token_id != self.word_to_idx[self.PAD_TOKEN] else 0 for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z987217vyod",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Model for Bot Detection\n",
    "class BotDetectionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.position_encoding = PositionalEncoding(config.d_model, config.max_seq_length)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                config.d_model, \n",
    "                config.num_heads, \n",
    "                config.d_ff, \n",
    "                config.dropout\n",
    "            ) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.d_model, config.num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights following BERT-style initialization\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def create_padding_mask(self, input_ids, pad_token_id=0):\n",
    "        \"\"\"Create padding mask for attention\"\"\"\n",
    "        return (input_ids != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # Token embeddings\n",
    "        embeddings = self.token_embedding(input_ids)\n",
    "        embeddings = embeddings * math.sqrt(self.config.d_model)  # Scale embeddings\n",
    "        \n",
    "        # Add positional encoding\n",
    "        embeddings = self.position_encoding(embeddings.transpose(0, 1)).transpose(0, 1)\n",
    "        \n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is not None:\n",
    "            # Convert to proper format for attention\n",
    "            mask = attention_mask.unsqueeze(1).unsqueeze(2).float()\n",
    "            mask = (1.0 - mask) * -1e9\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        hidden_states = embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states, mask)\n",
    "        \n",
    "        # Get [CLS] token representation for classification\n",
    "        cls_representation = hidden_states[:, 0, :]  # First token is [CLS]\n",
    "        \n",
    "        # Classification\n",
    "        cls_representation = self.layer_norm(cls_representation)\n",
    "        cls_representation = self.dropout(cls_representation)\n",
    "        logits = self.classifier(cls_representation)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing model architecture...\")\n",
    "test_model = BotDetectionTransformer(config).to(config.device)\n",
    "test_input = torch.randint(0, config.vocab_size, (2, config.max_seq_length)).to(config.device)\n",
    "test_mask = torch.ones_like(test_input).to(config.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output = test_model(test_input, test_mask)\n",
    "    print(f\"Model output shape: {test_output.shape}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "\n",
    "del test_model, test_input, test_mask  # Clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uxv53nar7bn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class for Cresci-2017\n",
    "class TwitterBotDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        token_ids = self.tokenizer.encode(text, self.max_length)\n",
    "        attention_mask = self.tokenizer.create_attention_mask(token_ids)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Data Loading and Preprocessing Functions\n",
    "def load_cresci_data_demo():\n",
    "    \\\"\\\"\\\"\n",
    "    Demo function to create synthetic data in Cresci-2017 format.\n",
    "    Replace this with actual Cresci-2017 data loading.\n",
    "    \\\"\\\"\\\"\n",
    "    print(\"Creating demo dataset (replace with actual Cresci-2017 data loading)\")\n",
    "    \n",
    "    # Synthetic bot tweets (common bot patterns)\n",
    "    bot_tweets = [\n",
    "        \"Follow me for amazing deals! #sponsored #ad #promotion\",\n",
    "        \"Click here for free money! Link in bio #scam #fake\",\n",
    "        \"RT @randomuser: Buy now! Limited time offer!!!\",\n",
    "        \"Amazing product! Everyone should buy this! #ad #promotion\",\n",
    "        \"Free followers! Click the link! #followers #fake\",\n",
    "        \"Best deals ever! Don't miss out! RT if you agree!\",\n",
    "        \"Automatic retweet service available! DM for details\",\n",
    "        \"Buy cheap followers and likes! Fast delivery guaranteed!\",\n",
    "        \"Promoting amazing products! Check my timeline! #ad\",\n",
    "        \"RT @sponsor: Limited time sale! Buy now or regret later!\"\n",
    "    ] * 100  # Repeat to create more samples\n",
    "    \n",
    "    # Synthetic human tweets (more natural patterns)\n",
    "    human_tweets = [\n",
    "        \"Just had an amazing coffee at my local cafe â˜•\",\n",
    "        \"Working from home today, feeling productive!\",\n",
    "        \"Can't wait for the weekend! Anyone have fun plans?\",\n",
    "        \"Just finished reading a great book, highly recommend it\",\n",
    "        \"Weather is beautiful today, perfect for a walk\",\n",
    "        \"Cooking dinner for my family tonight, trying new recipe\",\n",
    "        \"Great conversation with friends over lunch today\",\n",
    "        \"Learning something new every day, love continuous growth\",\n",
    "        \"Watching a documentary about ocean life, so fascinating\",\n",
    "        \"Planning my next vacation, so many places to explore\"\n",
    "    ] * 100  # Repeat to create more samples\n",
    "    \n",
    "    # Create labels (0 = human, 1 = bot)\n",
    "    texts = human_tweets + bot_tweets\n",
    "    labels = [0] * len(human_tweets) + [1] * len(bot_tweets)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': labels,\n",
    "        'account_type': ['human' if l == 0 else 'bot' for l in labels]\n",
    "    })\n",
    "    \n",
    "    print(f\"Created dataset with {len(df)} samples\")\n",
    "    print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_account_level_splits(df, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \\\"\\\"\\\"\n",
    "    Create account-level splits to prevent data leakage.\n",
    "    In real Cresci-2017, you would group by account_id and split at account level.\n",
    "    \\\"\\\"\\\"\n",
    "    print(\"Creating data splits...\")\n",
    "    \n",
    "    # For demo, we'll simulate account-level splitting\n",
    "    # In real implementation, group by account_id first\n",
    "    unique_indices = df.index.tolist()\n",
    "    \n",
    "    # Split indices (simulating account-level split)\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        unique_indices, test_size=test_size + val_size, \n",
    "        random_state=random_state, stratify=df.loc[unique_indices, 'label']\n",
    "    )\n",
    "    \n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx, test_size=test_size / (test_size + val_size),\n",
    "        random_state=random_state, stratify=df.loc[temp_idx, 'label']\n",
    "    )\n",
    "    \n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "    test_df = df.loc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Train size: {len(train_df)}\")\n",
    "    print(f\"Validation size: {len(val_df)}\")\n",
    "    print(f\"Test size: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading Cresci-2017 dataset...\")\n",
    "df = load_cresci_data_demo()\n",
    "\n",
    "# Create splits\n",
    "train_df, val_df, test_df = create_account_level_splits(df, config.test_size, config.val_size)\n",
    "\n",
    "# Build tokenizer on training data\n",
    "tokenizer = TwitterTokenizer(vocab_size=config.vocab_size)\n",
    "tokenizer.build_vocab(train_df['text'].tolist())\n",
    "\n",
    "print(f\"\\\\nVocabulary statistics:\")\n",
    "print(f\"- Total unique words in training: {len(tokenizer.word_counts)}\")\n",
    "print(f\"- Vocabulary size: {len(tokenizer.word_to_idx)}\")\n",
    "print(f\"- Most common words: {list(tokenizer.word_counts.most_common(10))}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TwitterBotDataset(\n",
    "    train_df['text'].tolist(), \n",
    "    train_df['label'].tolist(), \n",
    "    tokenizer, \n",
    "    config.max_seq_length\n",
    ")\n",
    "\n",
    "val_dataset = TwitterBotDataset(\n",
    "    val_df['text'].tolist(), \n",
    "    val_df['label'].tolist(), \n",
    "    tokenizer, \n",
    "    config.max_seq_length\n",
    ")\n",
    "\n",
    "test_dataset = TwitterBotDataset(\n",
    "    test_df['text'].tolist(), \n",
    "    test_df['label'].tolist(), \n",
    "    tokenizer, \n",
    "    config.max_seq_length\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\\\nData loaders created:\")\n",
    "print(f\"- Train batches: {len(train_loader)}\")\n",
    "print(f\"- Val batches: {len(val_loader)}\")\n",
    "print(f\"- Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vb0q1ro4jhr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Hyperparameters\n",
    "class Config:\n",
    "    # Model Architecture\n",
    "    d_model = 512          # Embedding dimension\n",
    "    num_layers = 10         # Number of transformer layers\n",
    "    num_heads = 12          # Number of attention heads\n",
    "    d_ff = 2048           # Feed forward hidden dimension\n",
    "    dropout = 0.1         # Dropout rate\n",
    "    max_seq_length = 256  # Maximum sequence length\n",
    "    vocab_size = 30000    # Vocabulary size\n",
    "    \n",
    "    # Training Parameters\n",
    "    batch_size = 32\n",
    "    learning_rate = 2e-5\n",
    "    warmup_steps = 500\n",
    "    max_epochs = 10\n",
    "    gradient_clip_norm = 1.0\n",
    "    \n",
    "    # Data\n",
    "    num_classes = 2       # Binary: bot vs human\n",
    "    test_size = 0.2\n",
    "    val_size = 0.1\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model configuration: d_model={config.d_model}, layers={config.num_layers}, heads={config.num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096e1cb",
   "metadata": {},
   "source": [
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97822810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64])\n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention.size: torch.Size([30, 8, 5, 5])\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0041, -0.1982,  0.0871,  ...,  0.1250, -0.1515, -0.0741],\n",
       "         [ 0.0665, -0.0146, -0.2594,  ...,  0.1805,  0.1107,  0.0254],\n",
       "         [-0.2314, -0.1776,  0.3157,  ..., -0.1198,  0.1174,  0.1548],\n",
       "         [-0.0467, -0.0217, -0.1965,  ...,  0.0164,  0.0309,  0.3131],\n",
       "         [-0.3640,  0.2524,  0.1880,  ..., -0.3888,  0.2219, -0.2219]],\n",
       "\n",
       "        [[ 0.0195, -0.1414,  0.0139,  ..., -0.1696,  0.1596,  0.4023],\n",
       "         [ 0.1171,  0.1215,  0.0429,  ...,  0.2675,  0.1349, -0.2392],\n",
       "         [-0.0793,  0.0033,  0.2842,  ..., -0.0579, -0.1722, -0.3303],\n",
       "         [ 0.0121, -0.1976, -0.3343,  ...,  0.0189,  0.2305, -0.1511],\n",
       "         [-0.2417, -0.2875,  0.2639,  ..., -0.2358,  0.2205,  0.0966]],\n",
       "\n",
       "        [[ 0.0671, -0.0536, -0.1049,  ...,  0.1113, -0.0619,  0.3027],\n",
       "         [ 0.1294,  0.0100,  0.0183,  ...,  0.0975,  0.0381, -0.1794],\n",
       "         [-0.0598, -0.0061, -0.0928,  ..., -0.3921, -0.2349,  0.3383],\n",
       "         [-0.1280,  0.2926,  0.0364,  ..., -0.0779, -0.2042,  0.2748],\n",
       "         [ 0.0036,  0.2452, -0.0958,  ...,  0.1174,  0.1669,  0.0178]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0444,  0.2963,  0.0503,  ..., -0.0307,  0.1378,  0.2250],\n",
       "         [ 0.0616,  0.0542, -0.2522,  ...,  0.0653, -0.2812,  0.1085],\n",
       "         [-0.0982, -0.0694, -0.1562,  ...,  0.1838, -0.2174,  0.2408],\n",
       "         [-0.0867,  0.0771, -0.0123,  ...,  0.1471, -0.0993,  0.1401],\n",
       "         [-0.1334, -0.1786,  0.0459,  ...,  0.1207, -0.0523, -0.1382]],\n",
       "\n",
       "        [[ 0.2878, -0.1576, -0.0892,  ...,  0.1166, -0.1121, -0.0037],\n",
       "         [-0.2096,  0.4403,  0.3353,  ..., -0.1052,  0.0632, -0.0732],\n",
       "         [ 0.0151, -0.2616, -0.0757,  ..., -0.1216, -0.1157, -0.0811],\n",
       "         [ 0.0651, -0.1333,  0.0553,  ...,  0.0078, -0.1591, -0.2170],\n",
       "         [-0.2046,  0.3249,  0.0115,  ...,  0.0885,  0.0666,  0.1550]],\n",
       "\n",
       "        [[ 0.1773, -0.0630,  0.0485,  ..., -0.0798, -0.0708,  0.1589],\n",
       "         [-0.1076, -0.0226, -0.1522,  ...,  0.0346, -0.1145,  0.1147],\n",
       "         [-0.1877,  0.1522, -0.0853,  ...,  0.2158, -0.1228, -0.3095],\n",
       "         [-0.0329,  0.3709,  0.1920,  ...,  0.0961, -0.2225,  0.1770],\n",
       "         [-0.1025,  0.3688,  0.1487,  ..., -0.1682, -0.2072, -0.3258]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model/inputs setup\n",
    "input_dim = 1024   # Input feature size per token\n",
    "d_model = 512      # Embedding/model size (must divide num_heads)\n",
    "num_heads = 8\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "\n",
    "# Create random input\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "\n",
    "# Instantiate MultiheadAttention class and run\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "output = model.forward(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2ce9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realFeel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
