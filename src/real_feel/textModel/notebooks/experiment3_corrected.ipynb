{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Corrected Cresci-2017 Data Loading & NN Architecture\n",
    "**VERIFIED AGAINST ACTUAL DATASET STRUCTURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERIFIED Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFIED CRESCI-2017 DATASET BREAKDOWN\n",
      "============================================================\n",
      "Usable datasets (with tweets): 6\n",
      "Total usable tweets: 6,637,623\n",
      "Total usable accounts: 12,737\n",
      "\n",
      "Usable datasets for text classification:\n",
      "  genuine_accounts      : 2,839,364 tweets ( 42.8%)\n",
      "  social_spambots_1     : 1,610,035 tweets ( 24.3%)\n",
      "  social_spambots_2     :  428,543 tweets (  6.5%)\n",
      "  social_spambots_3     : 1,418,558 tweets ( 21.4%)\n",
      "  traditional_spambots_1:  145,095 tweets (  2.2%)\n",
      "  fake_followers        :  196,028 tweets (  3.0%)\n",
      "\n",
      "Unusable datasets (no tweets):\n",
      "  traditional_spambots_2:  101 accounts only\n",
      "  traditional_spambots_3:  404 accounts only\n",
      "  traditional_spambots_4: 1129 accounts only\n",
      "\n",
      "Class imbalance (usable data only):\n",
      "  Largest: genuine_accounts (2.8M tweets, 43.3%)\n",
      "  Smallest: traditional_spambots_1 (145K tweets, 2.2%)\n",
      "  Imbalance ratio: 19.6:1\n"
     ]
    }
   ],
   "source": [
    "# VERIFIED dataset metadata from actual files\n",
    "CRESCI_DATASETS = {\n",
    "    # Datasets WITH tweets (text classification possible)\n",
    "    'genuine_accounts': {'accounts': 3474, 'tweets': 2839364, 'has_tweets': True, 'label': 0},\n",
    "    'social_spambots_1': {'accounts': 991, 'tweets': 1610035, 'has_tweets': True, 'label': 1},\n",
    "    'social_spambots_2': {'accounts': 3457, 'tweets': 428543, 'has_tweets': True, 'label': 2}, \n",
    "    'social_spambots_3': {'accounts': 464, 'tweets': 1418558, 'has_tweets': True, 'label': 3},\n",
    "    'traditional_spambots_1': {'accounts': 1000, 'tweets': 145095, 'has_tweets': True, 'label': 4},\n",
    "    'fake_followers': {'accounts': 3351, 'tweets': 196028, 'has_tweets': True, 'label': 5},\n",
    "    \n",
    "    # Datasets WITHOUT tweets (user metadata only - cannot use for text classification)\n",
    "    'traditional_spambots_2': {'accounts': 101, 'tweets': 0, 'has_tweets': False, 'label': 6},\n",
    "    'traditional_spambots_3': {'accounts': 404, 'tweets': 0, 'has_tweets': False, 'label': 7},\n",
    "    'traditional_spambots_4': {'accounts': 1129, 'tweets': 0, 'has_tweets': False, 'label': 8}\n",
    "}\n",
    "\n",
    "# Filter datasets that have tweets for text classification\n",
    "USABLE_DATASETS = {k: v for k, v in CRESCI_DATASETS.items() if v['has_tweets']}\n",
    "\n",
    "total_usable_tweets = sum(d['tweets'] for d in USABLE_DATASETS.values())\n",
    "total_usable_accounts = sum(d['accounts'] for d in USABLE_DATASETS.values())\n",
    "\n",
    "print(\"VERIFIED CRESCI-2017 DATASET BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Usable datasets (with tweets): {len(USABLE_DATASETS)}\")\n",
    "print(f\"Total usable tweets: {total_usable_tweets:,}\")\n",
    "print(f\"Total usable accounts: {total_usable_accounts:,}\")\n",
    "\n",
    "print(f\"\\nUsable datasets for text classification:\")\n",
    "for name, data in USABLE_DATASETS.items():\n",
    "    tweet_pct = data['tweets'] / total_usable_tweets * 100\n",
    "    print(f\"  {name:<22}: {data['tweets']:>8,} tweets ({tweet_pct:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nUnusable datasets (no tweets):\")\n",
    "for name, data in CRESCI_DATASETS.items():\n",
    "    if not data['has_tweets']:\n",
    "        print(f\"  {name:<22}: {data['accounts']:>4} accounts only\")\n",
    "\n",
    "print(f\"\\nClass imbalance (usable data only):\")\n",
    "print(f\"  Largest: genuine_accounts (2.8M tweets, 43.3%)\")\n",
    "print(f\"  Smallest: traditional_spambots_1 (145K tweets, 2.2%)\")\n",
    "print(f\"  Imbalance ratio: {2839364/145095:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing corrected data loader...\n",
      "\n",
      "Verifying dataset files:\n",
      "  ✓ genuine_accounts      : tweets.csv, users.csv\n",
      "  ✓ social_spambots_1     : tweets.csv, users.csv\n",
      "  ✓ social_spambots_2     : tweets.csv, users.csv\n",
      "  ✓ social_spambots_3     : tweets.csv, users.csv\n",
      "  ✓ traditional_spambots_1: tweets.csv, users.csv\n",
      "  ✓ fake_followers        : tweets.csv, users.csv\n",
      "  ✗ traditional_spambots_2: no tweets, users.csv\n",
      "  ✗ traditional_spambots_3: no tweets, users.csv\n",
      "  ✗ traditional_spambots_4: no tweets, users.csv\n"
     ]
    }
   ],
   "source": [
    "class CorrectedCresciDataLoader:\n",
    "    def __init__(self, data_root=\"../datasets/datasets_full.csv/\"):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.usable_datasets = USABLE_DATASETS  # Only datasets with tweets\n",
    "        \n",
    "    def verify_dataset_exists(self, dataset_name):\n",
    "        \"\"\"Verify dataset files exist before loading\"\"\"\n",
    "        tweets_path = self.data_root / f\"{dataset_name}.csv\" / \"tweets.csv\"\n",
    "        users_path = self.data_root / f\"{dataset_name}.csv\" / \"users.csv\"\n",
    "        \n",
    "        return {\n",
    "            'tweets_exists': tweets_path.exists(),\n",
    "            'users_exists': users_path.exists(),\n",
    "            'tweets_path': tweets_path,\n",
    "            'users_path': users_path\n",
    "        }\n",
    "    \n",
    "    def load_dataset_tweets(self, dataset_name, sample_size=None):\n",
    "        \"\"\"Load tweets from verified dataset with encoding handling\"\"\"\n",
    "        if dataset_name not in self.usable_datasets:\n",
    "            print(f\"Warning: {dataset_name} has no tweets - skipping\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        file_info = self.verify_dataset_exists(dataset_name)\n",
    "        \n",
    "        if not file_info['tweets_exists']:\n",
    "            print(f\"Error: tweets.csv not found for {dataset_name}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            print(f\"Loading {dataset_name}... \", end=\"\")\n",
    "            \n",
    "            # Try multiple encodings in order\n",
    "            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "            df = None\n",
    "            successful_encoding = None\n",
    "            \n",
    "            for encoding in encodings_to_try:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_info['tweets_path'], encoding=encoding, low_memory=False)\n",
    "                    successful_encoding = encoding\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    if encoding == encodings_to_try[-1]:  # Last encoding\n",
    "                        raise e\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                print(f\"✗ Error: Could not decode with any encoding\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"[{successful_encoding}] \", end=\"\")\n",
    "            \n",
    "            # Verify expected columns\n",
    "            if 'text' not in df.columns:\n",
    "                print(f\"Error: 'text' column not found in {dataset_name}\")\n",
    "                print(f\"Available columns: {df.columns.tolist()}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Clean data - handle encoding issues in text\n",
    "            original_size = len(df)\n",
    "            \n",
    "            # Remove rows with null text\n",
    "            df = df.dropna(subset=['text'])\n",
    "            \n",
    "            # Clean text encoding issues\n",
    "            def clean_text(text):\n",
    "                if not isinstance(text, str):\n",
    "                    return \"\"\n",
    "                try:\n",
    "                    # Remove non-printable characters and fix common encoding issues\n",
    "                    text = text.replace('\\x00', '').replace('\\r', ' ').replace('\\n', ' ').strip()\n",
    "                    # Ensure valid UTF-8 by encoding/decoding\n",
    "                    text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "                    return text if text else \"\"\n",
    "                except:\n",
    "                    return \"\"\n",
    "            \n",
    "            df['text'] = df['text'].apply(clean_text)\n",
    "            \n",
    "            # Remove empty texts after cleaning\n",
    "            df = df[df['text'] != '']\n",
    "            \n",
    "            # Sample if specified\n",
    "            if sample_size and len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "            # Add metadata\n",
    "            df['dataset'] = dataset_name\n",
    "            df['binary_label'] = 0 if dataset_name == 'genuine_accounts' else 1\n",
    "            df['multiclass_label'] = self.usable_datasets[dataset_name]['label']\n",
    "            \n",
    "            cleaned_size = len(df)\n",
    "            print(f\"✓ {cleaned_size:,} tweets (cleaned from {original_size:,})\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {str(e)[:50]}...\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def create_balanced_dataset(self, strategy='undersample', max_per_class=50000):\n",
    "        \"\"\"Create balanced dataset from usable datasets only\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        print(f\"\\nCreating balanced dataset with strategy: {strategy}\")\n",
    "        print(f\"Max samples per class: {max_per_class:,}\")\n",
    "        print(f\"Processing {len(self.usable_datasets)} datasets with tweets...\\n\")\n",
    "        \n",
    "        for dataset_name in self.usable_datasets.keys():\n",
    "            if strategy == 'undersample':\n",
    "                sample_size = min(max_per_class, self.usable_datasets[dataset_name]['tweets'])\n",
    "            else:\n",
    "                sample_size = None\n",
    "                \n",
    "            df = self.load_dataset_tweets(dataset_name, sample_size)\n",
    "            \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data loaded!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine all datasets\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\"FINAL DATASET SUMMARY\")\n",
    "        print(f\"=\"*50)\n",
    "        print(f\"Total tweets: {len(combined_df):,}\")\n",
    "        print(f\"\\nBinary distribution (Human=0, Bot=1):\")\n",
    "        binary_dist = combined_df['binary_label'].value_counts().sort_index()\n",
    "        for label, count in binary_dist.items():\n",
    "            label_name = \"Human\" if label == 0 else \"Bot\"\n",
    "            print(f\"  {label_name}: {count:,} ({count/len(combined_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nMulticlass distribution:\")\n",
    "        multi_dist = combined_df.groupby(['dataset', 'multiclass_label']).size().reset_index(name='count')\n",
    "        for _, row in multi_dist.iterrows():\n",
    "            print(f\"  {row['dataset']}: {row['count']:,} tweets (label {row['multiclass_label']})\")\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "# Test with actual data\n",
    "print(\"Initializing corrected data loader...\")\n",
    "data_loader = CorrectedCresciDataLoader()\n",
    "\n",
    "# Verify all datasets\n",
    "print(\"\\nVerifying dataset files:\")\n",
    "for dataset_name in CRESCI_DATASETS.keys():\n",
    "    file_info = data_loader.verify_dataset_exists(dataset_name)\n",
    "    status = \"✓\" if file_info['tweets_exists'] else \"✗\"\n",
    "    tweets_status = \"tweets.csv\" if file_info['tweets_exists'] else \"no tweets\"\n",
    "    users_status = \"users.csv\" if file_info['users_exists'] else \"no users\"\n",
    "    print(f\"  {status} {dataset_name:<22}: {tweets_status}, {users_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset with 5K samples per class...\n",
      "\n",
      "Creating balanced dataset with strategy: undersample\n",
      "Max samples per class: 5,000\n",
      "Processing 6 datasets with tweets...\n",
      "\n",
      "Loading genuine_accounts... [utf-8] ✓ 5,000 tweets (cleaned from 2,839,362)\n",
      "Loading social_spambots_1... [utf-8] ✓ 5,000 tweets (cleaned from 1,610,034)\n",
      "Loading social_spambots_2... [utf-8] ✓ 5,000 tweets (cleaned from 428,542)\n",
      "Loading social_spambots_3... [utf-8] ✓ 5,000 tweets (cleaned from 1,418,557)\n",
      "Loading traditional_spambots_1... [utf-8] ✓ 5,000 tweets (cleaned from 145,094)\n",
      "Loading fake_followers... [utf-8] ✓ 5,000 tweets (cleaned from 196,027)\n",
      "\n",
      "==================================================\n",
      "FINAL DATASET SUMMARY\n",
      "==================================================\n",
      "Total tweets: 30,000\n",
      "\n",
      "Binary distribution (Human=0, Bot=1):\n",
      "  Human: 5,000 (16.7%)\n",
      "  Bot: 25,000 (83.3%)\n",
      "\n",
      "Multiclass distribution:\n",
      "  fake_followers: 5,000 tweets (label 5)\n",
      "  genuine_accounts: 5,000 tweets (label 0)\n",
      "  social_spambots_1: 5,000 tweets (label 1)\n",
      "  social_spambots_2: 5,000 tweets (label 2)\n",
      "  social_spambots_3: 5,000 tweets (label 3)\n",
      "  traditional_spambots_1: 5,000 tweets (label 4)\n",
      "\n",
      "Sample tweets from the dataset:\n",
      "================================================================================\n",
      "\n",
      "GENUINE_ACCOUNTS examples:\n",
      "  1. @intanechigawa tastes quite childish. Not creamy at all. It's just okay.\n",
      "  2. Not sure I've ever managed to plug a USB cable in the right way round on the first attempt.\n",
      "\n",
      "SOCIAL_SPAMBOTS_1 examples:\n",
      "  1. Nessuno è una brutta persona, ma alcuni sono dei bei pirla.\n",
      "  2. #éiniziatal'estate! @ Ralf At Bikini http://t.co/zkp1gVFwuE\n",
      "\n",
      "SOCIAL_SPAMBOTS_2 examples:\n",
      "  1. I just wanted 2 share my happiness with u, n show u that there won't be any more heart breaks, You w...\n",
      "  2. Why tf are you even pissed at me lol\n",
      "\n",
      "Dataset ready for training with 30,000 tweets!\n"
     ]
    }
   ],
   "source": [
    "# Create small test dataset first\n",
    "print(\"Creating test dataset with 5K samples per class...\")\n",
    "cresci_df = data_loader.create_balanced_dataset(strategy='undersample', max_per_class=5000)\n",
    "\n",
    "# Show sample data if loaded successfully\n",
    "if not cresci_df.empty and len(cresci_df) > 0:\n",
    "    print(f\"\\nSample tweets from the dataset:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show examples from each class\n",
    "    for dataset_name in cresci_df['dataset'].unique()[:3]:  # Show first 3 datasets\n",
    "        sample_tweets = cresci_df[cresci_df['dataset'] == dataset_name]['text'].head(2)\n",
    "        print(f\"\\n{dataset_name.upper()} examples:\")\n",
    "        for i, tweet in enumerate(sample_tweets, 1):\n",
    "            clean_tweet = tweet.replace('\\n', ' ').replace('\\r', '')[:100]\n",
    "            print(f\"  {i}. {clean_tweet}{'...' if len(tweet) > 100 else ''}\")\n",
    "    \n",
    "    print(f\"\\nDataset ready for training with {len(cresci_df):,} tweets!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No data loaded - check dataset paths\")\n",
    "    print(\"Falling back to synthetic data for testing...\")\n",
    "    \n",
    "    # Create synthetic data for testing\n",
    "    synthetic_data = {\n",
    "        'text': [\n",
    "            \"Just had amazing coffee this morning ☕\",\n",
    "            \"Follow for follow! F4F! #followback\", \n",
    "            \"Beautiful day for a walk in the park\",\n",
    "            \"URGENT! Free money! Click now! #scam\",\n",
    "            \"Working from home today, very productive\",\n",
    "            \"Buy now! Limited time offer! Don't miss!\"\n",
    "        ],\n",
    "        'binary_label': [0, 1, 0, 1, 0, 1],\n",
    "        'multiclass_label': [0, 1, 0, 2, 0, 1],\n",
    "        'dataset': ['genuine_accounts', 'social_spambots_1', 'genuine_accounts', \n",
    "                   'social_spambots_2', 'genuine_accounts', 'social_spambots_1']\n",
    "    }\n",
    "    cresci_df = pd.DataFrame(synthetic_data)\n",
    "    print(f\"Created synthetic dataset with {len(cresci_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Model Architecture (Same as Before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration updated:\n",
      "  Binary classes: 2 (Human/Bot)\n",
      "  Multiclass classes: 6 (usable datasets only)\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "class OptimalBotConfig:\n",
    "    # Architecture (from our optimization)\n",
    "    d_model = 512\n",
    "    num_layers = 9\n",
    "    num_heads = 12\n",
    "    d_ff = 2048\n",
    "    dropout = 0.15\n",
    "    max_seq_length = 128\n",
    "    \n",
    "    # Tasks (corrected based on actual data)\n",
    "    num_binary_classes = 2      # Human vs Bot\n",
    "    num_multiclass_classes = 6  # Only usable datasets (not 9)\n",
    "    \n",
    "    # Training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = OptimalBotConfig()\n",
    "\n",
    "# [Previous model architecture code remains the same]\n",
    "# MultiHeadAttention, FeedForward, TransformerEncoderLayer, OptimizedBotDetector\n",
    "# ... [Include all the same model classes from previous experiment3] ...\n",
    "\n",
    "print(f\"Configuration updated:\")\n",
    "print(f\"  Binary classes: {config.num_binary_classes} (Human/Bot)\")\n",
    "print(f\"  Multiclass classes: {config.num_multiclass_classes} (usable datasets only)\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verified Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: 30000 samples for binary task\n",
      "\n",
      "Batch test successful:\n",
      "  Input shape: torch.Size([4, 128])\n",
      "  Attention mask shape: torch.Size([4, 128])\n",
      "  Labels: [1, 1, 0, 1]\n",
      "  Sample texts: ['\"Voci, risate, urla, gente. Ma l\\'unica cosa che sentiva era il rumore del suo cuore che, lentamente, si spezzava.\"', 'What would you do if...... Your on a bus to see a grou… — id kill the baby ab=nd then give it to dakotah in a sa… http://4ms.me/has2dk']\n",
      "\n",
      "Tokenization example:\n",
      "  Text: \"Voci, risate, urla, gente. Ma l'unica cosa che sentiva era ...\n",
      "  Tokens (43): ['\"', 'V', 'oci', ',', 'Ġris', 'ate', ',', 'Ġur']...\n",
      "  Padding efficiency: 45/128 tokens used (35.2%)\n"
     ]
    }
   ],
   "source": [
    "# Use Twitter-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "\n",
    "class VerifiedCresciDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128, task='binary'):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.binary_labels = dataframe['binary_label'].tolist()\n",
    "        self.multiclass_labels = dataframe['multiclass_label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.task = task\n",
    "        \n",
    "        print(f\"Dataset created: {len(self.texts)} samples for {task} task\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        if self.task == 'binary':\n",
    "            label = self.binary_labels[idx]\n",
    "        else:\n",
    "            label = self.multiclass_labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text  # Keep original for debugging\n",
    "        }\n",
    "\n",
    "# Create dataset and test\n",
    "if len(cresci_df) > 0:\n",
    "    dataset = VerifiedCresciDataset(cresci_df, tokenizer, task='binary')\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Test batch\n",
    "    sample_batch = next(iter(dataloader))\n",
    "    print(f\"\\nBatch test successful:\")\n",
    "    print(f\"  Input shape: {sample_batch['input_ids'].shape}\")\n",
    "    print(f\"  Attention mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "    print(f\"  Labels: {sample_batch['labels'].tolist()}\")\n",
    "    print(f\"  Sample texts: {sample_batch['text'][:2]}\")\n",
    "    \n",
    "    # Test tokenization efficiency\n",
    "    sample_text = sample_batch['text'][0]\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"\\nTokenization example:\")\n",
    "    print(f\"  Text: {sample_text[:60]}{'...' if len(sample_text) > 60 else ''}\")\n",
    "    print(f\"  Tokens ({len(tokens)}): {tokens[:8]}{'...' if len(tokens) > 8 else ''}\")\n",
    "    \n",
    "    # Show padding efficiency\n",
    "    actual_tokens = len([t for t in sample_batch['input_ids'][0] if t != tokenizer.pad_token_id])\n",
    "    efficiency = actual_tokens / config.max_seq_length * 100\n",
    "    print(f\"  Padding efficiency: {actual_tokens}/{config.max_seq_length} tokens used ({efficiency:.1f}%)\")\n",
    "else:\n",
    "    print(\"No data available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realFeel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
