{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model architecture\n",
    "optimized bot detection transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmartinez/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Model: 512d, 9L, 12H\n"
     ]
    }
   ],
   "source": [
    "class ModelConfig:\n",
    "    d_model = 512 # good amount\n",
    "    num_layers = 9 # captures bot patterns effectively\n",
    "    num_heads = 12 # dense attention diversity\n",
    "    d_ff = d_model * 4\n",
    "    vocab_size = 50000     # Subword tokenization\n",
    "    max_seq_length = 128 # twitter optimized\n",
    "    dropout = 0.15 # Higher for overfitting prevention\n",
    "    num_classes = 2 # bot or person\n",
    "    # will use ec2 for compute, might have to change into gpu\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = ModelConfig()\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Model: {config.d_model}d, {config.num_layers}L, {config.num_heads}H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## option 1: subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Follow me for amazing deals! #sponsored @user http://bit.ly/spam\n",
      "Tokens: ['Follow', 'Ġme', 'Ġfor', 'Ġamazing', 'Ġdeals', '!', 'Ġ#', 'sponsored', 'Ġ@', 'user', 'Ġhttp', '://', 'bit', '.', 'ly', '/', 'sp', 'am']\n",
      "[0, 18622, 162, 13, 2770, 2656, 328, 849, 16032, 787, 12105, 2054, 640, 5881, 4, 352, 73, 4182, 424, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "IDs shape: 128\n"
     ]
    }
   ],
   "source": [
    "# pre-trained twitter-aware tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "\n",
    "# test tokenization\n",
    "test_text = \"Follow me for amazing deals! #sponsored @user http://bit.ly/spam\"\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "token_ids = tokenizer.encode(test_text, max_length=config.max_seq_length, truncation=True, padding='max_length')\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"IDs shape: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## option 2: multi-task architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot logits shape: torch.Size([2, 2])\n",
      "Bot logits: tensor([[ 1.2711, -0.2371],\n",
      "        [-0.6507, -0.8039]], grad_fn=<AddmmBackward0>)\n",
      "Bot type logits shape: torch.Size([2, 6])\n",
      "Confidence shape: torch.Size([2, 1])\n",
      "Confidence: tensor([[0.2305],\n",
      "        [0.5401]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BotDetectionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.bot_classifier = nn.Linear(d_model, 2)  # bot/human\n",
    "        self.bot_type_classifier = nn.Linear(d_model, 6)  # bot categories\n",
    "        self.confidence_estimator = nn.Linear(d_model, 1)  # uncertainty\n",
    "        \n",
    "    def forward(self, cls_representation):\n",
    "        bot_logits = self.bot_classifier(cls_representation)\n",
    "        bot_type_logits = self.bot_type_classifier(cls_representation)\n",
    "        confidence = torch.sigmoid(self.confidence_estimator(cls_representation))\n",
    "        \n",
    "        return {\n",
    "            'bot_logits': bot_logits,\n",
    "            'bot_type_logits': bot_type_logits,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "# Test the head\n",
    "test_head = BotDetectionHead(config.d_model)\n",
    "test_input = torch.randn(2, config.d_model)\n",
    "outputs = test_head(test_input)\n",
    "print(f\"Bot logits shape: {outputs['bot_logits'].shape}\")\n",
    "print(f\"Bot logits: {outputs['bot_logits']}\")\n",
    "print(f\"Bot type logits shape: {outputs['bot_type_logits'].shape}\")\n",
    "print(f\"Confidence shape: {outputs['confidence'].shape}\")\n",
    "print(f\"Confidence: {outputs['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transformer_params(d_model, num_layers, num_heads, vocab_size, max_seq_length):\n",
    "    # Token embeddings\n",
    "    token_emb = vocab_size * d_model\n",
    "    \n",
    "    # Position embeddings\n",
    "    pos_emb = max_seq_length * d_model\n",
    "    \n",
    "    # Per transformer layer\n",
    "    attention_params = 4 * d_model * d_model  # Q,K,V,O projections\n",
    "    ffn_params = d_model * (d_model * 4) * 2  # Two linear layers\n",
    "    layer_norm_params = d_model * 2 * 2  # Two layer norms per layer\n",
    "    per_layer = attention_params + ffn_params + layer_norm_params\n",
    "    \n",
    "    total_layers = per_layer * num_layers\n",
    "    \n",
    "    # Classification head (simplified)\n",
    "    classifier = d_model * 2\n",
    "    \n",
    "    total = token_emb + pos_emb + total_layers + classifier\n",
    "    return total\n",
    "\n",
    "# Compare configurations\n",
    "old_params = calculate_transformer_params(768, 6, 8, 30000, 256)\n",
    "new_params = calculate_transformer_params(512, 9, 12, 50000, 128)\n",
    "\n",
    "print(f\"Old config (768d, 6L): {old_params:,} parameters\")\n",
    "print(f\"New config (512d, 9L): {new_params:,} parameters\")\n",
    "print(f\"Parameter ratio: {new_params/old_params:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realFeel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
