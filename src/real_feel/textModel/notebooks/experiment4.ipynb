{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Training Objective and Loss Function Design\n",
    "**Implementing Classification Head, Loss Function, and Pooling Strategy**\n",
    "\n",
    "Building on Experiment 3's data loading and model architecture to implement:\n",
    "1. **Classification Head**: Linear layer mapping from d_model to number of classes\n",
    "2. **Loss Function**: Cross-entropy with class weighting for imbalance handling\n",
    "3. **Pooling Strategy**: [CLS] token, mean pooling, and max pooling comparison\n",
    "4. **Training Pipeline**: Complete end-to-end training with proper evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmartinez/Developer/realFeelNEW/real-feel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Model Architecture\n",
    "**Theoretical Foundation**: Using optimized hyperparameters from previous experiments with focus on classification components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: 512d, 9L, 12H\n",
      "  Pooling: cls\n",
      "  Classes: 2\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "class TrainingConfig:\n",
    "    \"\"\"Complete configuration for training pipeline\"\"\"\n",
    "    \n",
    "    # Model Architecture (from experiment 3 optimization)\n",
    "    d_model = 512\n",
    "    num_layers = 9\n",
    "    num_heads = 12\n",
    "    d_ff = 2048\n",
    "    dropout = 0.15\n",
    "    max_seq_length = 128\n",
    "    \n",
    "    # Classification\n",
    "    num_classes = 2  # Binary: Human (0) vs Bot (1)\n",
    "    pooling_strategy = 'cls'  # Options: 'cls', 'mean', 'max', 'attention'\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 32\n",
    "    learning_rate = 2e-5\n",
    "    max_epochs = 10\n",
    "    warmup_steps = 1000\n",
    "    weight_decay = 0.01\n",
    "    gradient_clip_norm = 1.0\n",
    "    \n",
    "    # Class imbalance handling\n",
    "    use_class_weights = True\n",
    "    focal_loss_alpha = 0.25  # For focal loss alternative\n",
    "    focal_loss_gamma = 2.0\n",
    "    \n",
    "    # Data\n",
    "    vocab_size = 50265  # RoBERTa vocab size\n",
    "    test_size = 0.2\n",
    "    val_size = 0.1\n",
    "    \n",
    "    device = device\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {config.d_model}d, {config.num_layers}L, {config.num_heads}H\")\n",
    "print(f\"  Pooling: {config.pooling_strategy}\")\n",
    "print(f\"  Classes: {config.num_classes}\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Classification Head Implementation\n",
    "**Theoretical Justification**: \n",
    "- **Linear Classification**: Maps learned representations to class logits\n",
    "- **Multiple Pooling Strategies**: Different ways to aggregate token-level representations\n",
    "- **Dropout Regularization**: Prevents overfitting in classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification head test:\n",
      "  Input shape: torch.Size([2, 10, 512])\n",
      "  Output shape: torch.Size([2, 2])\n",
      "  Output logits: tensor([[ 1.1612, -0.3083],\n",
      "        [-0.7246,  0.1353]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiPoolingClassificationHead(nn.Module):\n",
    "    \"\"\"Advanced classification head with multiple pooling strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_classes, pooling_strategy='cls', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        \n",
    "        # Classification layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Different pooling strategies may require different input dims\n",
    "        if pooling_strategy == 'attention':\n",
    "            self.attention_weights = nn.Linear(d_model, 1)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "        \n",
    "        if hasattr(self, 'attention_weights'):\n",
    "            nn.init.normal_(self.attention_weights.weight, std=0.02)\n",
    "            nn.init.zeros_(self.attention_weights.bias)\n",
    "    \n",
    "    def pool_representations(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Pool token representations into single sequence representation\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: (batch_size, seq_len, d_model)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            pooled: (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        if self.pooling_strategy == 'cls':\n",
    "            # Use [CLS] token (first token)\n",
    "            pooled = hidden_states[:, 0, :]\n",
    "            \n",
    "        elif self.pooling_strategy == 'mean':\n",
    "            # Mean pooling with attention mask\n",
    "            if attention_mask is not None:\n",
    "                # Mask out padding tokens\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "                masked_hidden = hidden_states * mask_expanded\n",
    "                sum_hidden = masked_hidden.sum(dim=1)\n",
    "                sum_mask = attention_mask.sum(dim=1, keepdim=True)\n",
    "                pooled = sum_hidden / sum_mask\n",
    "            else:\n",
    "                pooled = hidden_states.mean(dim=1)\n",
    "                \n",
    "        elif self.pooling_strategy == 'max':\n",
    "            # Max pooling\n",
    "            if attention_mask is not None:\n",
    "                # Set padded positions to large negative value\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "                masked_hidden = hidden_states.clone()\n",
    "                masked_hidden[mask_expanded == 0] = -1e9\n",
    "                pooled = masked_hidden.max(dim=1)[0]\n",
    "            else:\n",
    "                pooled = hidden_states.max(dim=1)[0]\n",
    "                \n",
    "        elif self.pooling_strategy == 'attention':\n",
    "            # Attention-based pooling\n",
    "            attention_scores = self.attention_weights(hidden_states).squeeze(-1)\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n",
    "            \n",
    "            attention_probs = F.softmax(attention_scores, dim=1)\n",
    "            pooled = torch.bmm(attention_probs.unsqueeze(1), hidden_states).squeeze(1)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {self.pooling_strategy}\")\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of classification head\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: (batch_size, seq_len, d_model)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Pool representations\n",
    "        pooled = self.pool_representations(hidden_states, attention_mask)\n",
    "        \n",
    "        # Apply layer norm and dropout\n",
    "        pooled = self.layer_norm(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test classification head\n",
    "test_head = MultiPoolingClassificationHead(\n",
    "    d_model=config.d_model, \n",
    "    num_classes=config.num_classes,\n",
    "    pooling_strategy=config.pooling_strategy\n",
    ")\n",
    "\n",
    "# Test with dummy data\n",
    "batch_size, seq_len = 2, 10\n",
    "test_hidden = torch.randn(batch_size, seq_len, config.d_model)\n",
    "test_mask = torch.ones(batch_size, seq_len)\n",
    "test_mask[0, 7:] = 0  # Simulate padding\n",
    "\n",
    "test_logits = test_head(test_hidden, test_mask)\n",
    "print(f\"Classification head test:\")\n",
    "print(f\"  Input shape: {test_hidden.shape}\")\n",
    "print(f\"  Output shape: {test_logits.shape}\")\n",
    "print(f\"  Output logits: {test_logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Loss Functions\n",
    "**Theoretical Foundation**:\n",
    "- **Cross-Entropy**: Provides proper probabilistic interpretation, optimal for classification\n",
    "- **Class Weighting**: Compensates for unequal prior probabilities (class imbalance)\n",
    "- **Focal Loss**: Addresses class imbalance by down-weighting easy examples\n",
    "- **Label Smoothing**: Regularization technique to prevent overconfident predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function comparison on imbalanced data (90% class 0, 10% class 1):\n",
      "  standard_ce    : 1.0018\n",
      "  weighted_ce    : 0.7600\n",
      "  focal          : 0.1468\n",
      "  label_smoothed : 0.9947\n",
      "\n",
      "Computed class weights: tensor([0.5556, 5.0000])\n",
      "Weight ratio (class_1/class_0): 9.00\n"
     ]
    }
   ],
   "source": [
    "class AdvancedLossFunction(nn.Module):\n",
    "    \"\"\"Advanced loss function with multiple strategies for class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, class_weights=None, loss_type='weighted_ce', \n",
    "                 focal_alpha=0.25, focal_gamma=2.0, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_type = loss_type\n",
    "        self.focal_alpha = focal_alpha\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Register class weights as buffer\n",
    "        if class_weights is not None:\n",
    "            self.register_buffer('class_weights', class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "    \n",
    "    def compute_class_weights(self, labels):\n",
    "        \"\"\"\n",
    "        Compute class weights from labels using inverse frequency\n",
    "        \n",
    "        Args:\n",
    "            labels: Tensor of shape (N,)\n",
    "        \n",
    "        Returns:\n",
    "            class_weights: Tensor of shape (num_classes,)\n",
    "        \"\"\"\n",
    "        class_counts = torch.bincount(labels, minlength=self.num_classes).float()\n",
    "        total_samples = labels.size(0)\n",
    "        \n",
    "        # Inverse frequency weighting: w_i = N / (n_classes * n_i)\n",
    "        class_weights = total_samples / (self.num_classes * class_counts)\n",
    "        \n",
    "        # Handle zero counts (shouldn't happen in practice)\n",
    "        class_weights = torch.where(class_counts == 0, torch.zeros_like(class_weights), class_weights)\n",
    "        \n",
    "        return class_weights\n",
    "    \n",
    "    def weighted_cross_entropy(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Weighted cross-entropy loss\n",
    "        \n",
    "        Theory: L = -∑(w_i * y_i * log(p_i))\n",
    "        where w_i are class weights, y_i are true labels, p_i are predicted probabilities\n",
    "        \"\"\"\n",
    "        if self.class_weights is None:\n",
    "            self.class_weights = self.compute_class_weights(labels)\n",
    "        \n",
    "        return F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "    \n",
    "    def focal_loss(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Focal Loss for addressing class imbalance\n",
    "        \n",
    "        Theory: FL(p_t) = -α_t * (1-p_t)^γ * log(p_t)\n",
    "        where p_t is the model's estimated probability for the true class\n",
    "        \"\"\"\n",
    "        ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)  # p_t\n",
    "        \n",
    "        # Alpha weighting\n",
    "        if isinstance(self.focal_alpha, (list, tuple, torch.Tensor)):\n",
    "            alpha_t = self.focal_alpha[labels]\n",
    "        else:\n",
    "            alpha_t = self.focal_alpha\n",
    "        \n",
    "        focal_loss = alpha_t * (1 - pt) ** self.focal_gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "    \n",
    "    def label_smoothed_cross_entropy(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Cross-entropy with label smoothing\n",
    "        \n",
    "        Theory: Replaces hard targets with soft targets:\n",
    "        y_smooth = (1-ε) * y_true + ε/K\n",
    "        where ε is smoothing parameter, K is number of classes\n",
    "        \"\"\"\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Create smoothed labels\n",
    "        smooth_labels = torch.zeros_like(log_probs)\n",
    "        smooth_labels.fill_(self.label_smoothing / self.num_classes)\n",
    "        smooth_labels.scatter_(1, labels.unsqueeze(1), 1.0 - self.label_smoothing + self.label_smoothing / self.num_classes)\n",
    "        \n",
    "        loss = -torch.sum(smooth_labels * log_probs, dim=-1)\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Compute loss based on specified loss type\n",
    "        \n",
    "        Args:\n",
    "            logits: (batch_size, num_classes)\n",
    "            labels: (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "        \"\"\"\n",
    "        if self.loss_type == 'weighted_ce':\n",
    "            return self.weighted_cross_entropy(logits, labels)\n",
    "        elif self.loss_type == 'focal':\n",
    "            return self.focal_loss(logits, labels)\n",
    "        elif self.loss_type == 'label_smoothed':\n",
    "            return self.label_smoothed_cross_entropy(logits, labels)\n",
    "        elif self.loss_type == 'standard_ce':\n",
    "            return F.cross_entropy(logits, labels)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n",
    "\n",
    "# Test loss functions with imbalanced data\n",
    "test_logits = torch.randn(100, 2)\n",
    "# Create imbalanced labels (90% class 0, 10% class 1)\n",
    "test_labels = torch.cat([torch.zeros(90), torch.ones(10)]).long()\n",
    "\n",
    "# Test different loss functions\n",
    "loss_functions = {\n",
    "    'standard_ce': AdvancedLossFunction(2, loss_type='standard_ce'),\n",
    "    'weighted_ce': AdvancedLossFunction(2, loss_type='weighted_ce'),\n",
    "    'focal': AdvancedLossFunction(2, loss_type='focal'),\n",
    "    'label_smoothed': AdvancedLossFunction(2, loss_type='label_smoothed')\n",
    "}\n",
    "\n",
    "print(\"Loss function comparison on imbalanced data (90% class 0, 10% class 1):\")\n",
    "for name, loss_fn in loss_functions.items():\n",
    "    loss_value = loss_fn(test_logits, test_labels)\n",
    "    print(f\"  {name:15}: {loss_value.item():.4f}\")\n",
    "\n",
    "# Show class weight computation\n",
    "weighted_loss = loss_functions['weighted_ce']\n",
    "if hasattr(weighted_loss, 'class_weights') and weighted_loss.class_weights is not None:\n",
    "    print(f\"\\nComputed class weights: {weighted_loss.class_weights}\")\n",
    "    print(f\"Weight ratio (class_1/class_0): {weighted_loss.class_weights[1]/weighted_loss.class_weights[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Bot Detection Model with Training Objective\n",
    "**Integration**: Combining transformer backbone with advanced classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 165\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m total_params, trainable_params\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m model = \u001b[43mBotDetectionTransformerComplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m.to(config.device)\n\u001b[32m    166\u001b[39m total_params, trainable_params = model.get_num_parameters()\n\u001b[32m    168\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel created:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mBotDetectionTransformerComplete.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mself\u001b[39m.position_embedding = nn.Embedding(config.max_seq_length, config.d_model)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Transformer encoder\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.encoder_layers = nn.ModuleList([\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[43mTransformerEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.num_layers)\n\u001b[32m     90\u001b[39m ])\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Classification head\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mself\u001b[39m.classification_head = MultiPoolingClassificationHead(\n\u001b[32m     94\u001b[39m     config.d_model,\n\u001b[32m     95\u001b[39m     config.num_classes,\n\u001b[32m     96\u001b[39m     config.pooling_strategy,\n\u001b[32m     97\u001b[39m     config.dropout\n\u001b[32m     98\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mTransformerEncoderLayer.__init__\u001b[39m\u001b[34m(self, d_model, num_heads, d_ff, dropout)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model, num_heads, d_ff, dropout=\u001b[32m0.1\u001b[39m):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28mself\u001b[39m.self_attention = \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mself\u001b[39m.feed_forward = nn.Sequential(\n\u001b[32m     50\u001b[39m         nn.Linear(d_model, d_ff),\n\u001b[32m     51\u001b[39m         nn.ReLU(),\n\u001b[32m     52\u001b[39m         nn.Dropout(dropout),\n\u001b[32m     53\u001b[39m         nn.Linear(d_ff, d_model)\n\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mself\u001b[39m.norm1 = nn.LayerNorm(d_model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mMultiHeadAttention.__init__\u001b[39m\u001b[34m(self, d_model, num_heads, dropout)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model, num_heads, dropout=\u001b[32m0.1\u001b[39m):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m d_model % num_heads == \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.d_model = d_model\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.num_heads = num_heads\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention (from previous experiments)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.qkv_projection = nn.Linear(d_model, 3 * d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv_projection(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        \n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        attended_values = torch.matmul(attention_weights, v)\n",
    "        attended_values = attended_values.permute(0, 2, 1, 3).contiguous()\n",
    "        attended_values = attended_values.reshape(batch_size, seq_len, d_model)\n",
    "        \n",
    "        output = self.output_projection(attended_values)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Single transformer encoder layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.self_attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class BotDetectionTransformerComplete(nn.Module):\n",
    "    \"\"\"Complete bot detection transformer with advanced classification head\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.position_embedding = nn.Embedding(config.max_seq_length, config.d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                config.d_model, \n",
    "                config.num_heads, \n",
    "                config.d_ff, \n",
    "                config.dropout\n",
    "            ) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classification_head = MultiPoolingClassificationHead(\n",
    "            config.d_model,\n",
    "            config.num_classes,\n",
    "            config.pooling_strategy,\n",
    "            config.dropout\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights following BERT-style initialization\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # Create position ids\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        # Scale embeddings\n",
    "        embeddings = embeddings * math.sqrt(self.config.d_model)\n",
    "        \n",
    "        # Create attention mask for transformer\n",
    "        if attention_mask is not None:\n",
    "            # Convert to 4D mask for multi-head attention\n",
    "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_attention_mask = extended_attention_mask.to(dtype=embeddings.dtype)\n",
    "            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        hidden_states = embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states, extended_attention_mask)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classification_head(hidden_states, attention_mask)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        \"\"\"Get number of parameters\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params\n",
    "\n",
    "# Create model\n",
    "model = BotDetectionTransformerComplete(config).to(config.device)\n",
    "total_params, trainable_params = model.get_num_parameters()\n",
    "\n",
    "print(f\"Model created:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB (float32)\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input_ids = torch.randint(0, config.vocab_size, (2, 32)).to(config.device)\n",
    "test_attention_mask = torch.ones_like(test_input_ids).to(config.device)\n",
    "test_attention_mask[0, 20:] = 0  # Simulate padding\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_input_ids, test_attention_mask)\n",
    "    test_probs = F.softmax(test_output, dim=-1)\n",
    "\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"  Input shape: {test_input_ids.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Output logits: {test_output}\")\n",
    "print(f\"  Output probabilities: {test_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline with Comprehensive Evaluation\n",
    "**Training Objective**: Minimize classification error while handling class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BotDetectionTrainer:\n",
    "    \"\"\"Complete training pipeline for bot detection transformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Setup loss function\n",
    "        self.setup_loss_function()\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.setup_optimizer()\n",
    "        \n",
    "        # Setup scheduler\n",
    "        self.setup_scheduler()\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_precision': [],\n",
    "            'val_recall': [],\n",
    "            'val_f1': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_f1 = 0.0\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def setup_loss_function(self):\n",
    "        \"\"\"Setup loss function with class weights\"\"\"\n",
    "        if self.config.use_class_weights:\n",
    "            # Compute class weights from training data\n",
    "            all_labels = []\n",
    "            for batch in self.train_loader:\n",
    "                all_labels.extend(batch['labels'].tolist())\n",
    "            \n",
    "            # Convert to tensor and compute weights\n",
    "            labels_tensor = torch.tensor(all_labels)\n",
    "            \n",
    "            self.criterion = AdvancedLossFunction(\n",
    "                num_classes=self.config.num_classes,\n",
    "                loss_type='weighted_ce'\n",
    "            )\n",
    "            \n",
    "            # Compute and store class weights\n",
    "            class_weights = self.criterion.compute_class_weights(labels_tensor).to(self.device)\n",
    "            self.criterion.class_weights = class_weights\n",
    "            \n",
    "            print(f\"Class weights computed: {class_weights}\")\n",
    "        else:\n",
    "            self.criterion = AdvancedLossFunction(\n",
    "                num_classes=self.config.num_classes,\n",
    "                loss_type='standard_ce'\n",
    "            )\n",
    "    \n",
    "    def setup_optimizer(self):\n",
    "        \"\"\"Setup AdamW optimizer\"\"\"\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "    \n",
    "    def setup_scheduler(self):\n",
    "        \"\"\"Setup learning rate scheduler with warmup\"\"\"\n",
    "        total_steps = len(self.train_loader) * self.config.max_epochs\n",
    "        \n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < self.config.warmup_steps:\n",
    "                return current_step / self.config.warmup_steps\n",
    "            else:\n",
    "                return max(0.0, (total_steps - current_step) / (total_steps - self.config.warmup_steps))\n",
    "        \n",
    "        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = self.model(input_ids, attention_mask)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_norm)\n",
    "            \n",
    "            # Update weights\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def evaluate(self, data_loader, split_name=\"Val\"):\n",
    "        \"\"\"Evaluate model on given data loader\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(data_loader, desc=f\"Evaluating {split_name}\", leave=False)\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = self.model(input_ids, attention_mask)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                \n",
    "                # Get predictions and probabilities\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                # Store results\n",
    "                total_loss += loss.item()\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # Bot probability\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_predictions, average='binary'\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(all_labels, all_probabilities)\n",
    "        except ValueError:\n",
    "            roc_auc = 0.0\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'predictions': all_predictions,\n",
    "            'labels': all_labels,\n",
    "            'probabilities': all_probabilities\n",
    "        }\n",
    "    \n",
    "    def print_epoch_results(self, epoch, train_loss, val_metrics):\n",
    "        \"\"\"Print formatted epoch results\"\"\"\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.config.max_epochs}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss:   {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Val Acc:    {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val Prec:   {val_metrics['precision']:.4f}\")\n",
    "        print(f\"Val Rec:    {val_metrics['recall']:.4f}\")\n",
    "        print(f\"Val F1:     {val_metrics['f1']:.4f}\")\n",
    "        print(f\"Val AUC:    {val_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"LR:         {self.scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    def save_best_model(self, val_f1):\n",
    "        \"\"\"Save model if it's the best so far\"\"\"\n",
    "        if val_f1 > self.best_val_f1:\n",
    "            self.best_val_f1 = val_f1\n",
    "            self.best_model_state = self.model.state_dict().copy()\n",
    "            print(f\"★ New best model saved (F1: {val_f1:.4f})\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Complete training loop\"\"\"\n",
    "        print(f\"Starting training on {self.device}\")\n",
    "        print(f\"Training samples: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"Validation samples: {len(self.val_loader.dataset)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.evaluate(self.val_loader, \"Val\")\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_metrics['loss'])\n",
    "            self.history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "            self.history['val_precision'].append(val_metrics['precision'])\n",
    "            self.history['val_recall'].append(val_metrics['recall'])\n",
    "            self.history['val_f1'].append(val_metrics['f1'])\n",
    "            self.history['learning_rates'].append(self.scheduler.get_last_lr()[0])\n",
    "            \n",
    "            # Print results\n",
    "            self.print_epoch_results(epoch, train_loss, val_metrics)\n",
    "            \n",
    "            # Save best model\n",
    "            self.save_best_model(val_metrics['f1'])\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"\\nLoaded best model (F1: {self.best_val_f1:.4f})\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training curves\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Training History', fontsize=16)\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(self.history['train_loss'], label='Train', color='blue')\n",
    "        axes[0, 0].plot(self.history['val_loss'], label='Validation', color='red')\n",
    "        axes[0, 0].set_title('Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # F1 Score\n",
    "        axes[0, 1].plot(self.history['val_f1'], label='Validation F1', color='green')\n",
    "        axes[0, 1].set_title('F1 Score')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('F1 Score')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Precision and Recall\n",
    "        axes[1, 0].plot(self.history['val_precision'], label='Precision', color='purple')\n",
    "        axes[1, 0].plot(self.history['val_recall'], label='Recall', color='orange')\n",
    "        axes[1, 0].set_title('Precision and Recall')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Learning Rate\n",
    "        axes[1, 1].plot(self.history['learning_rates'], label='Learning Rate', color='brown')\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Training pipeline ready!\")\n",
    "print(f\"Configuration summary:\")\n",
    "print(f\"  Model: {config.d_model}d, {config.num_layers}L, {config.num_heads}H\")\n",
    "print(f\"  Pooling: {config.pooling_strategy}\")\n",
    "print(f\"  Loss: {'Weighted CE' if config.use_class_weights else 'Standard CE'}\")\n",
    "print(f\"  Optimizer: AdamW (lr={config.learning_rate}, wd={config.weight_decay})\")\n",
    "print(f\"  Training: {config.max_epochs} epochs, batch_size={config.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pooling Strategy Comparison\n",
    "**Experimental Analysis**: Compare different pooling strategies on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pooling_strategies(hidden_states, attention_mask, strategies=['cls', 'mean', 'max', 'attention']):\n",
    "    \"\"\"\n",
    "    Compare different pooling strategies on the same input\n",
    "    \n",
    "    Args:\n",
    "        hidden_states: (batch_size, seq_len, d_model)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        strategies: List of pooling strategies to compare\n",
    "    \n",
    "    Returns:\n",
    "        dict: Pooled representations for each strategy\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        head = MultiPoolingClassificationHead(\n",
    "            d_model=hidden_states.size(-1),\n",
    "            num_classes=2,\n",
    "            pooling_strategy=strategy\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pooled = head.pool_representations(hidden_states, attention_mask)\n",
    "            results[strategy] = pooled\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test pooling strategies\n",
    "batch_size, seq_len, d_model = 2, 10, config.d_model\n",
    "test_hidden = torch.randn(batch_size, seq_len, d_model)\n",
    "test_mask = torch.ones(batch_size, seq_len)\n",
    "test_mask[0, 7:] = 0  # Simulate padding\n",
    "test_mask[1, 9:] = 0  # Different padding\n",
    "\n",
    "pooling_results = compare_pooling_strategies(test_hidden, test_mask)\n",
    "\n",
    "print(\"Pooling strategy comparison:\")\n",
    "print(f\"Input shape: {test_hidden.shape}\")\n",
    "print(f\"Attention mask: {test_mask}\")\n",
    "print()\n",
    "\n",
    "for strategy, pooled in pooling_results.items():\n",
    "    print(f\"{strategy.upper()} pooling:\")\n",
    "    print(f\"  Output shape: {pooled.shape}\")\n",
    "    print(f\"  Sample values: {pooled[0, :5]}\")\n",
    "    print(f\"  Norm: {torch.norm(pooled, dim=-1)}\")\n",
    "    print()\n",
    "\n",
    "# Visualize differences between pooling strategies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Pooling Strategy Representations (First 20 dimensions)', fontsize=14)\n",
    "\n",
    "strategies = ['cls', 'mean', 'max', 'attention']\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for i, (strategy, color) in enumerate(zip(strategies, colors)):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    representation = pooling_results[strategy][0, :20].numpy()  # First sample, first 20 dims\n",
    "    ax.bar(range(20), representation, color=color, alpha=0.7)\n",
    "    ax.set_title(f'{strategy.upper()} Pooling')\n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Theoretical analysis\n",
    "print(\"Theoretical Analysis of Pooling Strategies:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. CLS Pooling:\")\n",
    "print(\"   - Uses dedicated [CLS] token trained for sequence classification\")\n",
    "print(\"   - Theoretically optimal for sequence-level tasks\")\n",
    "print(\"   - Requires [CLS] token in input sequence\")\n",
    "print()\n",
    "print(\"2. Mean Pooling:\")\n",
    "print(\"   - Averages all token representations (respecting padding)\")\n",
    "print(\"   - Preserves information from all tokens equally\")\n",
    "print(\"   - May dilute important signal with less relevant tokens\")\n",
    "print()\n",
    "print(\"3. Max Pooling:\")\n",
    "print(\"   - Takes maximum value across sequence for each dimension\")\n",
    "print(\"   - Captures most salient features\")\n",
    "print(\"   - May lose global sequence information\")\n",
    "print()\n",
    "print(\"4. Attention Pooling:\")\n",
    "print(\"   - Learns attention weights to combine token representations\")\n",
    "print(\"   - Most flexible, can learn optimal combination\")\n",
    "print(\"   - Requires additional parameters and training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Synthetic Data Testing (if real data unavailable)\n",
    "**Fallback Strategy**: Test training pipeline with synthetic bot/human patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset for testing if real data is unavailable\n",
    "class SyntheticBotDataset(Dataset):\n",
    "    \"\"\"Synthetic dataset for testing training pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, num_samples=1000, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Generate synthetic bot and human patterns\n",
    "        self.texts, self.labels = self._generate_synthetic_data(num_samples)\n",
    "        \n",
    "    def _generate_synthetic_data(self, num_samples):\n",
    "        \"\"\"Generate synthetic bot and human tweets\"\"\"\n",
    "        \n",
    "        # Bot patterns (obvious spam/promotional content)\n",
    "        bot_patterns = [\n",
    "            \"Follow me for amazing deals! #sponsored #ad #promotion {}\",\n",
    "            \"Click here for free money! Link in bio #scam #fake {}\",\n",
    "            \"RT @randomuser: Buy now! Limited time offer!!! {}\",\n",
    "            \"Amazing product! Everyone should buy this! #ad #promotion {}\",\n",
    "            \"Free followers! Click the link! #followers #fake {}\",\n",
    "            \"Best deals ever! Don't miss out! RT if you agree! {}\",\n",
    "            \"Automatic retweet service available! DM for details {}\",\n",
    "            \"Buy cheap followers and likes! Fast delivery guaranteed! {}\",\n",
    "            \"Promoting amazing products! Check my timeline! #ad {}\",\n",
    "            \"RT @sponsor: Limited time sale! Buy now or regret later! {}\",\n",
    "            \"Get rich quick! This one trick will change your life! {}\",\n",
    "            \"Follow for follow! F4F! Gain followers fast! {}\",\n",
    "            \"Limited offer! Buy our product now! Link in bio! {}\",\n",
    "            \"Automatic engagement service! Boost your social media! {}\",\n",
    "            \"Special discount code! Use SAVE50 for 50% off! {}\"\n",
    "        ]\n",
    "        \n",
    "        # Human patterns (natural conversational content)\n",
    "        human_patterns = [\n",
    "            \"Just had an amazing coffee at my local cafe ☕ {}\",\n",
    "            \"Working from home today, feeling productive! {}\",\n",
    "            \"Can't wait for the weekend! Anyone have fun plans? {}\",\n",
    "            \"Just finished reading a great book, highly recommend it {}\",\n",
    "            \"Weather is beautiful today, perfect for a walk {}\",\n",
    "            \"Cooking dinner for my family tonight, trying new recipe {}\",\n",
    "            \"Great conversation with friends over lunch today {}\",\n",
    "            \"Learning something new every day, love continuous growth {}\",\n",
    "            \"Watching a documentary about ocean life, so fascinating {}\",\n",
    "            \"Planning my next vacation, so many places to explore {}\",\n",
    "            \"Had a wonderful day at the park with my kids {}\",\n",
    "            \"Excited about the new project I'm working on {}\",\n",
    "            \"Morning jog completed! Feeling energized for the day {}\",\n",
    "            \"Reading an interesting article about renewable energy {}\",\n",
    "            \"Enjoying a quiet evening with a good book {}\"\n",
    "        ]\n",
    "        \n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        # Generate samples (50% human, 50% bot for balanced dataset)\n",
    "        for i in range(num_samples):\n",
    "            if i < num_samples // 2:\n",
    "                # Human tweets\n",
    "                pattern = np.random.choice(human_patterns)\n",
    "                text = pattern.format(f\"sample_{i}\")\n",
    "                label = 0\n",
    "            else:\n",
    "                # Bot tweets\n",
    "                pattern = np.random.choice(bot_patterns)\n",
    "                text = pattern.format(f\"sample_{i}\")\n",
    "                label = 1\n",
    "            \n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "        \n",
    "        # Shuffle\n",
    "        combined = list(zip(texts, labels))\n",
    "        np.random.shuffle(combined)\n",
    "        texts, labels = zip(*combined)\n",
    "        \n",
    "        return list(texts), list(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create synthetic dataset and data loaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "\n",
    "# Create datasets\n",
    "full_dataset = SyntheticBotDataset(tokenizer, num_samples=2000, max_length=config.max_seq_length)\n",
    "\n",
    "# Split into train/val/test\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.2 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Synthetic dataset created:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Show sample data\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in sample_batch['input_ids'][:3]]\n",
    "sample_labels = sample_batch['labels'][:3]\n",
    "\n",
    "print(f\"\\nSample data:\")\n",
    "for i, (text, label) in enumerate(zip(sample_texts, sample_labels)):\n",
    "    label_name = \"Human\" if label == 0 else \"Bot\"\n",
    "    print(f\"  {i+1}. [{label_name}] {text[:80]}{'...' if len(text) > 80 else ''}\")\n",
    "\n",
    "# Check class balance\n",
    "all_labels = []\n",
    "for batch in train_loader:\n",
    "    all_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "class_counts = np.bincount(all_labels)\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"  Human (0): {class_counts[0]} ({class_counts[0]/len(all_labels)*100:.1f}%)\")\n",
    "print(f\"  Bot (1): {class_counts[1]} ({class_counts[1]/len(all_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Execution with Different Configurations\n",
    "**Experiment**: Compare different pooling strategies and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different configurations\n",
    "experiments = [\n",
    "    {\n",
    "        'name': 'CLS_Weighted',\n",
    "        'pooling_strategy': 'cls',\n",
    "        'use_class_weights': True,\n",
    "        'max_epochs': 3  # Quick experiment\n",
    "    },\n",
    "    {\n",
    "        'name': 'Mean_Weighted',\n",
    "        'pooling_strategy': 'mean',\n",
    "        'use_class_weights': True,\n",
    "        'max_epochs': 3\n",
    "    },\n",
    "    {\n",
    "        'name': 'Attention_Weighted',\n",
    "        'pooling_strategy': 'attention',\n",
    "        'use_class_weights': True,\n",
    "        'max_epochs': 3\n",
    "    }\n",
    "]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: {experiment['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Update config for this experiment\n",
    "    exp_config = TrainingConfig()\n",
    "    exp_config.pooling_strategy = experiment['pooling_strategy']\n",
    "    exp_config.use_class_weights = experiment['use_class_weights']\n",
    "    exp_config.max_epochs = experiment['max_epochs']\n",
    "    \n",
    "    # Create model\n",
    "    model = BotDetectionTransformerComplete(exp_config).to(exp_config.device)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = BotDetectionTrainer(model, train_loader, val_loader, exp_config)\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        trained_model = trainer.train()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        test_metrics = trainer.evaluate(test_loader, \"Test\")\n",
    "        \n",
    "        # Store results\n",
    "        experiment_results.append({\n",
    "            'name': experiment['name'],\n",
    "            'pooling_strategy': experiment['pooling_strategy'],\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_precision': test_metrics['precision'],\n",
    "            'test_recall': test_metrics['recall'],\n",
    "            'test_f1': test_metrics['f1'],\n",
    "            'test_roc_auc': test_metrics['roc_auc'],\n",
    "            'best_val_f1': trainer.best_val_f1\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nTest Results for {experiment['name']}:\")\n",
    "        print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"  F1: {test_metrics['f1']:.4f}\")\n",
    "        print(f\"  ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Show classification report\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        report = classification_report(\n",
    "            test_metrics['labels'], \n",
    "            test_metrics['predictions'],\n",
    "            target_names=['Human', 'Bot'],\n",
    "            digits=4\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment {experiment['name']}: {e}\")\n",
    "        experiment_results.append({\n",
    "            'name': experiment['name'],\n",
    "            'pooling_strategy': experiment['pooling_strategy'],\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# Compare results\n",
    "if experiment_results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPERIMENT COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results_df = pd.DataFrame([r for r in experiment_results if 'error' not in r])\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Find best configuration\n",
    "        best_config = results_df.loc[results_df['test_f1'].idxmax()]\n",
    "        print(f\"\\nBest Configuration: {best_config['name']}\")\n",
    "        print(f\"  Pooling Strategy: {best_config['pooling_strategy']}\")\n",
    "        print(f\"  Test F1: {best_config['test_f1']:.4f}\")\n",
    "        print(f\"  Test Accuracy: {best_config['test_accuracy']:.4f}\")\n",
    "        print(f\"  Test ROC-AUC: {best_config['test_roc_auc']:.4f}\")\n",
    "    \n",
    "    # Show errors if any\n",
    "    error_results = [r for r in experiment_results if 'error' in r]\n",
    "    if error_results:\n",
    "        print(f\"\\nFailed Experiments:\")\n",
    "        for result in error_results:\n",
    "            print(f\"  {result['name']}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Analysis and Theoretical Summary\n",
    "**Analysis**: Understanding the learned representations and training dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical Summary and Analysis\n",
    "print(\"EXPERIMENT 4: TRAINING OBJECTIVE AND LOSS FUNCTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. CLASSIFICATION HEAD DESIGN:\")\n",
    "print(\"  ✓ Multi-pooling strategies implemented (CLS, Mean, Max, Attention)\")\n",
    "print(\"  ✓ Linear classification layer with proper initialization\")\n",
    "print(\"  ✓ Dropout regularization to prevent overfitting\")\n",
    "print(\"  ✓ Layer normalization for training stability\")\n",
    "\n",
    "print(\"\\n2. LOSS FUNCTION IMPLEMENTATION:\")\n",
    "print(\"  ✓ Cross-entropy with class weighting for imbalance handling\")\n",
    "print(\"  ✓ Focal loss alternative for extreme imbalance cases\")\n",
    "print(\"  ✓ Label smoothing for regularization\")\n",
    "print(\"  ✓ Automatic class weight computation from training data\")\n",
    "\n",
    "print(\"\\n3. POOLING STRATEGY EVALUATION:\")\n",
    "strategies_analysis = {\n",
    "    'cls': {\n",
    "        'theory': 'Dedicated sequence classification token',\n",
    "        'pros': 'Optimal for classification tasks, BERT-style',\n",
    "        'cons': 'Requires CLS token in input sequence'\n",
    "    },\n",
    "    'mean': {\n",
    "        'theory': 'Average all token representations',\n",
    "        'pros': 'Uses all tokens equally, simple and effective',\n",
    "        'cons': 'May dilute important signals'\n",
    "    },\n",
    "    'max': {\n",
    "        'theory': 'Maximum activation across sequence',\n",
    "        'pros': 'Captures most salient features',\n",
    "        'cons': 'May lose global context'\n",
    "    },\n",
    "    'attention': {\n",
    "        'theory': 'Learned attention weights for combination',\n",
    "        'pros': 'Most flexible, learns optimal combination',\n",
    "        'cons': 'Additional parameters, more complex'\n",
    "    }\n",
    "}\n",
    "\n",
    "for strategy, analysis in strategies_analysis.items():\n",
    "    print(f\"  {strategy.upper()}:\")\n",
    "    print(f\"    Theory: {analysis['theory']}\")\n",
    "    print(f\"    Pros: {analysis['pros']}\")\n",
    "    print(f\"    Cons: {analysis['cons']}\")\n",
    "\n",
    "print(\"\\n4. TRAINING PIPELINE FEATURES:\")\n",
    "print(\"  ✓ AdamW optimizer with weight decay\")\n",
    "print(\"  ✓ Learning rate scheduling with warmup\")\n",
    "print(\"  ✓ Gradient clipping for stability\")\n",
    "print(\"  ✓ Early stopping based on validation F1\")\n",
    "print(\"  ✓ Comprehensive evaluation metrics\")\n",
    "\n",
    "print(\"\\n5. THEORETICAL JUSTIFICATIONS:\")\n",
    "print(\"  • Cross-entropy provides proper probabilistic interpretation\")\n",
    "print(\"  • Class weighting compensates for unequal prior probabilities\")\n",
    "print(\"  • [CLS] token pooling is theoretically optimal for sequence classification\")\n",
    "print(\"  • Transformer architecture captures long-range dependencies in text\")\n",
    "print(\"  • Multi-head attention provides diverse representation spaces\")\n",
    "\n",
    "print(\"\\n6. IMPLEMENTATION HIGHLIGHTS:\")\n",
    "print(\"  • Modular design allows easy experimentation with components\")\n",
    "print(\"  • Proper handling of padding tokens in all pooling strategies\")\n",
    "print(\"  • Comprehensive evaluation with multiple metrics\")\n",
    "print(\"  • Synthetic data generation for testing when real data unavailable\")\n",
    "print(\"  • Automatic class weight computation from training distribution\")\n",
    "\n",
    "if experiment_results and any('error' not in r for r in experiment_results):\n",
    "    print(\"\\n7. EXPERIMENTAL RESULTS:\")\n",
    "    successful_results = [r for r in experiment_results if 'error' not in r]\n",
    "    if successful_results:\n",
    "        best_result = max(successful_results, key=lambda x: x['test_f1'])\n",
    "        print(f\"  Best performing configuration: {best_result['name']}\")\n",
    "        print(f\"  Best F1 score: {best_result['test_f1']:.4f}\")\n",
    "        print(f\"  Best pooling strategy: {best_result['pooling_strategy']}\")\n",
    "\n",
    "print(\"\\n8. NEXT STEPS FOR PRODUCTION:\")\n",
    "print(\"  • Integration with real Cresci-2017 dataset\")\n",
    "print(\"  • Hyperparameter optimization (learning rate, batch size, etc.)\")\n",
    "print(\"  • Model distillation for deployment efficiency\")\n",
    "print(\"  • Ensemble methods combining different pooling strategies\")\n",
    "print(\"  • Active learning for continuous model improvement\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXPERIMENT 4 COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realFeel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
